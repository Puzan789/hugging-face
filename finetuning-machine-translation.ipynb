{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport transformers","metadata":{"_uuid":"4e1a8971-c75d-4753-8e76-29e276f26929","_cell_guid":"272c2f38-450b-47a2-a3cc-9ac9292a5196","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:46:22.227518Z","iopub.execute_input":"2024-03-20T08:46:22.228189Z","iopub.status.idle":"2024-03-20T08:46:22.237237Z","shell.execute_reply.started":"2024-03-20T08:46:22.228154Z","shell.execute_reply":"2024-03-20T08:46:22.236264Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as  pd\n\nimport torch\nimport transformers\n\nfrom datasets import Dataset\nfrom datasets import load_metric\n\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForSeq2SeqLM\nfrom transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer","metadata":{"_uuid":"7dc7cdd5-9a93-4df1-b30b-32bfc27781cd","_cell_guid":"c4f6d526-5778-41fd-95ea-907ea29e7d64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T09:29:17.293045Z","iopub.execute_input":"2024-03-20T09:29:17.293621Z","iopub.status.idle":"2024-03-20T09:29:17.302963Z","shell.execute_reply.started":"2024-03-20T09:29:17.293588Z","shell.execute_reply":"2024-03-20T09:29:17.300844Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"\n# Read Parquet file into a DataFrame\ndf_train = pd.read_parquet('/kaggle/working/Nepali-Roman-Transliteration/data/train-00000-of-00001.parquet')\ndf_val = pd.read_parquet('/kaggle/working/Nepali-Roman-Transliteration/data/validation-00000-of-00001.parquet')","metadata":{"_uuid":"11677a84-ca45-4424-89e5-037a018d9c58","_cell_guid":"2ae71fad-e74b-44f4-9a92-d88dc41ccde0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:46:25.596165Z","iopub.execute_input":"2024-03-20T08:46:25.596616Z","iopub.status.idle":"2024-03-20T08:46:29.183809Z","shell.execute_reply.started":"2024-03-20T08:46:25.596586Z","shell.execute_reply":"2024-03-20T08:46:29.182670Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"combined_df = pd.concat([df_train, df_val], ignore_index=True)","metadata":{"_uuid":"f65418b1-c286-4880-be2c-cce5816b43fc","_cell_guid":"15d94f77-31d8-4044-b423-8856b42d7391","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:47:25.773959Z","iopub.execute_input":"2024-03-20T08:47:25.774725Z","iopub.status.idle":"2024-03-20T08:47:25.850268Z","shell.execute_reply.started":"2024-03-20T08:47:25.774691Z","shell.execute_reply":"2024-03-20T08:47:25.849155Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"combined_df","metadata":{"_uuid":"853f23a7-f766-46ab-9724-14346e7b30d2","_cell_guid":"eeb66676-449a-4bd6-bb4f-712561f20efb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:47:27.625824Z","iopub.execute_input":"2024-03-20T08:47:27.626699Z","iopub.status.idle":"2024-03-20T08:47:27.642050Z","shell.execute_reply.started":"2024-03-20T08:47:27.626663Z","shell.execute_reply":"2024-03-20T08:47:27.641027Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"        unique_identifier    native word      english word\n0                    nep1   मुस्कुराउँदै      muskuraundai\n1                    nep2       मान्दछन्        mandachhan\n2                    nep3          भएझैं         bhaejhain\n3                    nep4         हराउँछ       haraaunchha\n4                    nep5         मुन्टो             munto\n...                   ...            ...               ...\n2400213           nep2800         शीतलता          shitalta\n2400214           nep2801  ट्राउजरमाथिको    trausermathiko\n2400215           nep2802       शेखरजीका       shekharjika\n2400216           nep2803       हवाइवेमा         hawaiwema\n2400217           nep2804  आठबिसकोटस्थित  athabisakotsthit\n\n[2400218 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_identifier</th>\n      <th>native word</th>\n      <th>english word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nep1</td>\n      <td>मुस्कुराउँदै</td>\n      <td>muskuraundai</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nep2</td>\n      <td>मान्दछन्</td>\n      <td>mandachhan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nep3</td>\n      <td>भएझैं</td>\n      <td>bhaejhain</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nep4</td>\n      <td>हराउँछ</td>\n      <td>haraaunchha</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nep5</td>\n      <td>मुन्टो</td>\n      <td>munto</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2400213</th>\n      <td>nep2800</td>\n      <td>शीतलता</td>\n      <td>shitalta</td>\n    </tr>\n    <tr>\n      <th>2400214</th>\n      <td>nep2801</td>\n      <td>ट्राउजरमाथिको</td>\n      <td>trausermathiko</td>\n    </tr>\n    <tr>\n      <th>2400215</th>\n      <td>nep2802</td>\n      <td>शेखरजीका</td>\n      <td>shekharjika</td>\n    </tr>\n    <tr>\n      <th>2400216</th>\n      <td>nep2803</td>\n      <td>हवाइवेमा</td>\n      <td>hawaiwema</td>\n    </tr>\n    <tr>\n      <th>2400217</th>\n      <td>nep2804</td>\n      <td>आठबिसकोटस्थित</td>\n      <td>athabisakotsthit</td>\n    </tr>\n  </tbody>\n</table>\n<p>2400218 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 8\nBLEU = \"bleu\"\nENGLISH = \"roman\"\nENGLISH_TEXT = \"native word\"\nEPOCH = \"epoch\"\nINPUT_IDS = \"input_ids\"\nMAX_INPUT_LENGTH = 128\nMAX_TARGET_LENGTH = 128\nMODEL_CHECKPOINT = \"GenzNepal/mt5-summarize-nepali\"\nLABELS = \"labels\"\nPREFIX = \"\"\nPORTUGUESE = \"ne\"\nPORTUGUESE_TEXT = \"english word\"\nSCORE = \"score\"\nSOURCE_LANG = \"ne\"\nTARGET_LANG = \"roman\"\nTRANSLATION = \"translation\"\nUNNAMED_COL = \"Unnamed: 0\"\nMODEL_NAME = MODEL_CHECKPOINT.split(\"/\")[-1]","metadata":{"_uuid":"b16b2bc5-cd9a-488c-bc35-eb0646e409c9","_cell_guid":"e1ab4ba7-5c1d-4629-a4a0-3e84420fb6ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T09:17:32.327971Z","iopub.execute_input":"2024-03-20T09:17:32.328388Z","iopub.status.idle":"2024-03-20T09:17:32.335996Z","shell.execute_reply.started":"2024-03-20T09:17:32.328333Z","shell.execute_reply":"2024-03-20T09:17:32.335067Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"X=combined_df[\"native word\"]\nY=combined_df['english word']","metadata":{"_uuid":"472b70d3-f3ca-4ca0-9c20-c23f5615fb47","_cell_guid":"2dc0281d-faab-437a-9833-cf8c8aa43944","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:47:32.307965Z","iopub.execute_input":"2024-03-20T08:47:32.308610Z","iopub.status.idle":"2024-03-20T08:47:32.433519Z","shell.execute_reply.started":"2024-03-20T08:47:32.308565Z","shell.execute_reply":"2024-03-20T08:47:32.432337Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"_uuid":"28bd7cf0-ae15-48f4-bc56-303baf50ffb6","_cell_guid":"3194281d-910f-47c8-abcc-1fa6185353e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:47:35.132124Z","iopub.execute_input":"2024-03-20T08:47:35.132847Z","iopub.status.idle":"2024-03-20T08:47:35.138276Z","shell.execute_reply.started":"2024-03-20T08:47:35.132813Z","shell.execute_reply":"2024-03-20T08:47:35.137145Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"def postprocess_text(preds: list, labels: list) -> tuple:\n    \"\"\"Performs post processing on the prediction text and labels\"\"\"\n\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\n\ndef prep_data_for_model_fine_tuning(source_lang: list, target_lang: list) -> list:\n    \"\"\"Takes the input data lists and converts into translation list of dicts\"\"\"\n\n    data_dict = dict()\n    data_dict[TRANSLATION] = []\n\n    for sr_text, tr_text in zip(source_lang, target_lang):\n        temp_dict = dict()\n        temp_dict[PORTUGUESE] = sr_text\n        temp_dict[ENGLISH] = tr_text\n\n        data_dict[TRANSLATION].append(temp_dict)\n\n    return data_dict\n\n\ndef generate_model_ready_dataset(dataset: list, source: str, target: str,\n                                 model_checkpoint: str,\n                                 tokenizer: AutoTokenizer):\n    \"\"\"Makes the data training ready for the model\"\"\"\n\n    preped_data = []\n\n    for row in dataset:\n        inputs = PREFIX + row[source]\n        targets = row[target]\n\n        model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH,\n                                 truncation=True, padding=True)\n\n        model_inputs[TRANSLATION] = row\n\n        # setup the tokenizer for targets\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(targets, max_length=MAX_INPUT_LENGTH,\n                                 truncation=True, padding=True)\n            model_inputs[LABELS] = labels[INPUT_IDS]\n\n        preped_data.append(model_inputs)\n\n    return preped_data\n\n\n\ndef compute_metrics(eval_preds: tuple) -> dict:\n    \"\"\"computes bleu score and other performance metrics \"\"\"\n\n    metric = load_metric(\"sacrebleu\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n\n    preds, labels = eval_preds\n\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {BLEU: result[SCORE]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n\n    result[GEN_LEN] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n\n    return result","metadata":{"_uuid":"d7a577fb-5359-46db-9670-fbade2a02360","_cell_guid":"7c62408e-e517-4b37-8082-2863332b49e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:50:04.426772Z","iopub.execute_input":"2024-03-20T08:50:04.427198Z","iopub.status.idle":"2024-03-20T08:50:04.444500Z","shell.execute_reply.started":"2024-03-20T08:50:04.427151Z","shell.execute_reply":"2024-03-20T08:50:04.443382Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, shuffle=True, random_state=100)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.10, shuffle=True, random_state=100)","metadata":{"_uuid":"4b81dea3-c134-485f-a66e-68ae96b5a704","_cell_guid":"3ad8ce5f-64bc-485f-ba78-432078e6f652","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:48:53.307684Z","iopub.execute_input":"2024-03-20T08:48:53.308048Z","iopub.status.idle":"2024-03-20T08:48:54.294071Z","shell.execute_reply.started":"2024-03-20T08:48:53.308018Z","shell.execute_reply":"2024-03-20T08:48:54.292879Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"print(\"INITIAL X-TRAIN SHAPE: \", x_train.shape)\nprint(\"INITIAL Y-TRAIN SHAPE: \", y_train.shape)\nprint(\"X-TEST SHAPE: \", x_test.shape)\nprint(\"Y-TEST SHAPE: \", y_test.shape)","metadata":{"_uuid":"51a3fd65-385c-4019-816e-f51f0b157310","_cell_guid":"2e86cf6f-ac31-4431-9386-47a6c541d4ed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:47:56.055642Z","iopub.execute_input":"2024-03-20T08:47:56.056323Z","iopub.status.idle":"2024-03-20T08:47:56.064965Z","shell.execute_reply.started":"2024-03-20T08:47:56.056286Z","shell.execute_reply":"2024-03-20T08:47:56.063837Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"INITIAL X-TRAIN SHAPE:  (1944176,)\nINITIAL Y-TRAIN SHAPE:  (1944176,)\nX-TEST SHAPE:  (240022,)\nY-TEST SHAPE:  (240022,)\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)","metadata":{"_uuid":"562dfea2-4379-4ad7-ae9a-abbf49907194","_cell_guid":"1707dcf9-4e28-4364-9fb6-2b714b2d1e45","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:50:31.184273Z","iopub.execute_input":"2024-03-20T08:50:31.185185Z","iopub.status.idle":"2024-03-20T08:50:31.410153Z","shell.execute_reply.started":"2024-03-20T08:50:31.185148Z","shell.execute_reply":"2024-03-20T08:50:31.409039Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"training_data = prep_data_for_model_fine_tuning(x_train.values, y_train.values)\n\nvalidation_data = prep_data_for_model_fine_tuning(x_val.values, y_val.values)\n\ntest_data = prep_data_for_model_fine_tuning(x_test.values, y_test.values)","metadata":{"_uuid":"b4d2cbf3-7a1f-4ab1-9756-45b7467a6cca","_cell_guid":"cd03670b-0ba1-4403-ad05-61aea21dd696","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:51:00.915276Z","iopub.execute_input":"2024-03-20T08:51:00.915965Z","iopub.status.idle":"2024-03-20T08:51:03.165373Z","shell.execute_reply.started":"2024-03-20T08:51:00.915933Z","shell.execute_reply":"2024-03-20T08:51:03.164188Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"train_data = generate_model_ready_dataset(dataset=training_data[TRANSLATION],\n                                          tokenizer=tokenizer,\n                                          source=PORTUGUESE,\n                                          target=ENGLISH,\n                                          model_checkpoint=MODEL_CHECKPOINT)\n\nvalidation_data = generate_model_ready_dataset(dataset=validation_data[TRANSLATION],\n                                               tokenizer=tokenizer,\n                                               source=PORTUGUESE,\n                                               target=ENGLISH,\n                                               model_checkpoint=MODEL_CHECKPOINT)\n\ntest_data = generate_model_ready_dataset(dataset=test_data[TRANSLATION],\n                                               tokenizer=tokenizer,\n                                               source=PORTUGUESE,\n                                               target=ENGLISH,\n                                               model_checkpoint=MODEL_CHECKPOINT)","metadata":{"_uuid":"ff436a27-c571-4569-b1d4-da3d9267de90","_cell_guid":"d2c3bd5a-b640-4f9d-bd3e-e63a18f027c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T08:51:11.929577Z","iopub.execute_input":"2024-03-20T08:51:11.929958Z","iopub.status.idle":"2024-03-20T08:58:28.985981Z","shell.execute_reply.started":"2024-03-20T08:51:11.929919Z","shell.execute_reply":"2024-03-20T08:58:28.984672Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.DataFrame.from_records(train_data)\nvalidation_df = pd.DataFrame.from_records(validation_data)\ntest_df = pd.DataFrame.from_records(test_data)\n\n# Convert DataFrames to Dataset objects\ntrain_dataset = Dataset.from_pandas(train_df)\nvalidation_dataset = Dataset.from_pandas(validation_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T08:59:30.227805Z","iopub.execute_input":"2024-03-20T08:59:30.228221Z","iopub.status.idle":"2024-03-20T08:59:53.949523Z","shell.execute_reply.started":"2024-03-20T08:59:30.228189Z","shell.execute_reply":"2024-03-20T08:59:53.948411Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\nfor param in model.encoder.parameters():  # Accessing the encoder of MT5 model\n    param.requires_grad = False\n\nmodel_args = Seq2SeqTrainingArguments(\n    f\"{MODEL_NAME}-finetuned-{SOURCE_LANG}-to-{TARGET_LANG}\",\n    evaluation_strategy=EPOCH,\n    learning_rate=2e-4,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    weight_decay=0.02,\n    save_total_limit=3,\n    num_train_epochs=1,\n    predict_with_generate=True\n)\n\n# Create a data collator for sequence-to-sequence tasks\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"_uuid":"70fee1ae-7c4e-4170-ac9b-d018f97353af","_cell_guid":"4cdebb80-f610-49cf-aad2-dc271809817b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T09:36:40.158492Z","iopub.execute_input":"2024-03-20T09:36:40.159157Z","iopub.status.idle":"2024-03-20T09:36:44.281109Z","shell.execute_reply.started":"2024-03-20T09:36:40.159119Z","shell.execute_reply":"2024-03-20T09:36:44.279940Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"import torch\nimport time\nimport gc\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    gc.collect()\n\ndef wait_until_enough_gpu_memory(min_memory_available, max_retries=10, sleep_time=5):\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n\n    for _ in range(max_retries):\n        info = nvmlDeviceGetMemoryInfo(handle)\n        if info.free >= min_memory_available:\n            break\n        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n        time.sleep(sleep_time)\n    else:\n        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")\n\n# Usage example\nmin_memory_available = 2 * 1024 * 1024 * 1024  # 2GB\nclear_gpu_memory()\nwait_until_enough_gpu_memory(min_memory_available)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T09:21:32.707065Z","iopub.execute_input":"2024-03-20T09:21:32.707518Z","iopub.status.idle":"2024-03-20T09:21:37.596744Z","shell.execute_reply.started":"2024-03-20T09:21:32.707486Z","shell.execute_reply":"2024-03-20T09:21:37.595723Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"# Initialize the Seq2SeqTrainer for fine-tuning\ntrainer = Seq2SeqTrainer(\n    model,\n    model_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Commence the model training\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(\"FineTunedTransformer\")","metadata":{"_uuid":"607aa45a-d24f-4c1e-964e-c1bea67ed724","_cell_guid":"eb24ddd6-316f-48eb-88ce-43ad55c1c427","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-20T09:44:54.798894Z","iopub.execute_input":"2024-03-20T09:44:54.799864Z","iopub.status.idle":"2024-03-20T09:44:55.006890Z","shell.execute_reply.started":"2024-03-20T09:44:54.799826Z","shell.execute_reply":"2024-03-20T09:44:55.005082Z"},"trusted":true},"execution_count":138,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[138], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the Seq2SeqTrainer for fine-tuning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Commence the model training\u001b[39;00m\n\u001b[1;32m     13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:56\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     44\u001b[0m     model: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m ):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:489\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    488\u001b[0m ):\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:730\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 730\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 490.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.17 GiB is free. Process 5792 has 7.57 GiB memory in use. Of the allocated memory 6.55 GiB is allocated by PyTorch, and 847.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 490.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.17 GiB is free. Process 5792 has 7.57 GiB memory in use. Of the allocated memory 6.55 GiB is allocated by PyTorch, and 847.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name, param.requires_grad)\n    # Check if the parameter corresponds to the encoder block 0 layer\n    if name.startswith(\"encoder.block.0.layer\"): \n        param.requires_grad = True  # Unfreeze the parameter\n        print(name, param.requires_grad)\n    # Check if the parameter corresponds to any of the decoder block layers from 0 to 6\n    for i in range(7):\n        if name.startswith(f\"decoder.block.{i}.layer\"):\n            param.requires_grad = True  # Unfreeze the parameter\n            print(name, param.requires_grad)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T09:47:51.301365Z","iopub.execute_input":"2024-03-20T09:47:51.302284Z","iopub.status.idle":"2024-03-20T09:47:51.383393Z","shell.execute_reply.started":"2024-03-20T09:47:51.302249Z","shell.execute_reply":"2024-03-20T09:47:51.382384Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stdout","text":"shared.weight False\nencoder.block.0.layer.0.SelfAttention.q.weight False\nencoder.block.0.layer.0.SelfAttention.q.weight True\nencoder.block.0.layer.0.SelfAttention.k.weight False\nencoder.block.0.layer.0.SelfAttention.k.weight True\nencoder.block.0.layer.0.SelfAttention.v.weight False\nencoder.block.0.layer.0.SelfAttention.v.weight True\nencoder.block.0.layer.0.SelfAttention.o.weight False\nencoder.block.0.layer.0.SelfAttention.o.weight True\nencoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\nencoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight True\nencoder.block.0.layer.0.layer_norm.weight False\nencoder.block.0.layer.0.layer_norm.weight True\nencoder.block.0.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.0.layer.1.DenseReluDense.wi_0.weight True\nencoder.block.0.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.0.layer.1.DenseReluDense.wi_1.weight True\nencoder.block.0.layer.1.DenseReluDense.wo.weight False\nencoder.block.0.layer.1.DenseReluDense.wo.weight True\nencoder.block.0.layer.1.layer_norm.weight False\nencoder.block.0.layer.1.layer_norm.weight True\nencoder.block.1.layer.0.SelfAttention.q.weight False\nencoder.block.1.layer.0.SelfAttention.k.weight False\nencoder.block.1.layer.0.SelfAttention.v.weight False\nencoder.block.1.layer.0.SelfAttention.o.weight False\nencoder.block.1.layer.0.layer_norm.weight False\nencoder.block.1.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.1.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.1.layer.1.DenseReluDense.wo.weight False\nencoder.block.1.layer.1.layer_norm.weight False\nencoder.block.2.layer.0.SelfAttention.q.weight False\nencoder.block.2.layer.0.SelfAttention.k.weight False\nencoder.block.2.layer.0.SelfAttention.v.weight False\nencoder.block.2.layer.0.SelfAttention.o.weight False\nencoder.block.2.layer.0.layer_norm.weight False\nencoder.block.2.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.2.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.2.layer.1.DenseReluDense.wo.weight False\nencoder.block.2.layer.1.layer_norm.weight False\nencoder.block.3.layer.0.SelfAttention.q.weight False\nencoder.block.3.layer.0.SelfAttention.k.weight False\nencoder.block.3.layer.0.SelfAttention.v.weight False\nencoder.block.3.layer.0.SelfAttention.o.weight False\nencoder.block.3.layer.0.layer_norm.weight False\nencoder.block.3.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.3.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.3.layer.1.DenseReluDense.wo.weight False\nencoder.block.3.layer.1.layer_norm.weight False\nencoder.block.4.layer.0.SelfAttention.q.weight False\nencoder.block.4.layer.0.SelfAttention.k.weight False\nencoder.block.4.layer.0.SelfAttention.v.weight False\nencoder.block.4.layer.0.SelfAttention.o.weight False\nencoder.block.4.layer.0.layer_norm.weight False\nencoder.block.4.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.4.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.4.layer.1.DenseReluDense.wo.weight False\nencoder.block.4.layer.1.layer_norm.weight False\nencoder.block.5.layer.0.SelfAttention.q.weight False\nencoder.block.5.layer.0.SelfAttention.k.weight False\nencoder.block.5.layer.0.SelfAttention.v.weight False\nencoder.block.5.layer.0.SelfAttention.o.weight False\nencoder.block.5.layer.0.layer_norm.weight False\nencoder.block.5.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.5.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.5.layer.1.DenseReluDense.wo.weight False\nencoder.block.5.layer.1.layer_norm.weight False\nencoder.block.6.layer.0.SelfAttention.q.weight False\nencoder.block.6.layer.0.SelfAttention.k.weight False\nencoder.block.6.layer.0.SelfAttention.v.weight False\nencoder.block.6.layer.0.SelfAttention.o.weight False\nencoder.block.6.layer.0.layer_norm.weight False\nencoder.block.6.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.6.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.6.layer.1.DenseReluDense.wo.weight False\nencoder.block.6.layer.1.layer_norm.weight False\nencoder.block.7.layer.0.SelfAttention.q.weight False\nencoder.block.7.layer.0.SelfAttention.k.weight False\nencoder.block.7.layer.0.SelfAttention.v.weight False\nencoder.block.7.layer.0.SelfAttention.o.weight False\nencoder.block.7.layer.0.layer_norm.weight False\nencoder.block.7.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.7.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.7.layer.1.DenseReluDense.wo.weight False\nencoder.block.7.layer.1.layer_norm.weight False\nencoder.final_layer_norm.weight False\ndecoder.block.0.layer.0.SelfAttention.q.weight False\ndecoder.block.0.layer.0.SelfAttention.q.weight True\ndecoder.block.0.layer.0.SelfAttention.k.weight False\ndecoder.block.0.layer.0.SelfAttention.k.weight True\ndecoder.block.0.layer.0.SelfAttention.v.weight False\ndecoder.block.0.layer.0.SelfAttention.v.weight True\ndecoder.block.0.layer.0.SelfAttention.o.weight False\ndecoder.block.0.layer.0.SelfAttention.o.weight True\ndecoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\ndecoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight True\ndecoder.block.0.layer.0.layer_norm.weight False\ndecoder.block.0.layer.0.layer_norm.weight True\ndecoder.block.0.layer.1.EncDecAttention.q.weight False\ndecoder.block.0.layer.1.EncDecAttention.q.weight True\ndecoder.block.0.layer.1.EncDecAttention.k.weight False\ndecoder.block.0.layer.1.EncDecAttention.k.weight True\ndecoder.block.0.layer.1.EncDecAttention.v.weight False\ndecoder.block.0.layer.1.EncDecAttention.v.weight True\ndecoder.block.0.layer.1.EncDecAttention.o.weight False\ndecoder.block.0.layer.1.EncDecAttention.o.weight True\ndecoder.block.0.layer.1.layer_norm.weight False\ndecoder.block.0.layer.1.layer_norm.weight True\ndecoder.block.0.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.0.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.0.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.0.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.0.layer.2.DenseReluDense.wo.weight False\ndecoder.block.0.layer.2.DenseReluDense.wo.weight True\ndecoder.block.0.layer.2.layer_norm.weight False\ndecoder.block.0.layer.2.layer_norm.weight True\ndecoder.block.1.layer.0.SelfAttention.q.weight False\ndecoder.block.1.layer.0.SelfAttention.q.weight True\ndecoder.block.1.layer.0.SelfAttention.k.weight False\ndecoder.block.1.layer.0.SelfAttention.k.weight True\ndecoder.block.1.layer.0.SelfAttention.v.weight False\ndecoder.block.1.layer.0.SelfAttention.v.weight True\ndecoder.block.1.layer.0.SelfAttention.o.weight False\ndecoder.block.1.layer.0.SelfAttention.o.weight True\ndecoder.block.1.layer.0.layer_norm.weight False\ndecoder.block.1.layer.0.layer_norm.weight True\ndecoder.block.1.layer.1.EncDecAttention.q.weight False\ndecoder.block.1.layer.1.EncDecAttention.q.weight True\ndecoder.block.1.layer.1.EncDecAttention.k.weight False\ndecoder.block.1.layer.1.EncDecAttention.k.weight True\ndecoder.block.1.layer.1.EncDecAttention.v.weight False\ndecoder.block.1.layer.1.EncDecAttention.v.weight True\ndecoder.block.1.layer.1.EncDecAttention.o.weight False\ndecoder.block.1.layer.1.EncDecAttention.o.weight True\ndecoder.block.1.layer.1.layer_norm.weight False\ndecoder.block.1.layer.1.layer_norm.weight True\ndecoder.block.1.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.1.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.1.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.1.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.1.layer.2.DenseReluDense.wo.weight False\ndecoder.block.1.layer.2.DenseReluDense.wo.weight True\ndecoder.block.1.layer.2.layer_norm.weight False\ndecoder.block.1.layer.2.layer_norm.weight True\ndecoder.block.2.layer.0.SelfAttention.q.weight False\ndecoder.block.2.layer.0.SelfAttention.q.weight True\ndecoder.block.2.layer.0.SelfAttention.k.weight False\ndecoder.block.2.layer.0.SelfAttention.k.weight True\ndecoder.block.2.layer.0.SelfAttention.v.weight False\ndecoder.block.2.layer.0.SelfAttention.v.weight True\ndecoder.block.2.layer.0.SelfAttention.o.weight False\ndecoder.block.2.layer.0.SelfAttention.o.weight True\ndecoder.block.2.layer.0.layer_norm.weight False\ndecoder.block.2.layer.0.layer_norm.weight True\ndecoder.block.2.layer.1.EncDecAttention.q.weight False\ndecoder.block.2.layer.1.EncDecAttention.q.weight True\ndecoder.block.2.layer.1.EncDecAttention.k.weight False\ndecoder.block.2.layer.1.EncDecAttention.k.weight True\ndecoder.block.2.layer.1.EncDecAttention.v.weight False\ndecoder.block.2.layer.1.EncDecAttention.v.weight True\ndecoder.block.2.layer.1.EncDecAttention.o.weight False\ndecoder.block.2.layer.1.EncDecAttention.o.weight True\ndecoder.block.2.layer.1.layer_norm.weight False\ndecoder.block.2.layer.1.layer_norm.weight True\ndecoder.block.2.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.2.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.2.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.2.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.2.layer.2.DenseReluDense.wo.weight False\ndecoder.block.2.layer.2.DenseReluDense.wo.weight True\ndecoder.block.2.layer.2.layer_norm.weight False\ndecoder.block.2.layer.2.layer_norm.weight True\ndecoder.block.3.layer.0.SelfAttention.q.weight False\ndecoder.block.3.layer.0.SelfAttention.q.weight True\ndecoder.block.3.layer.0.SelfAttention.k.weight False\ndecoder.block.3.layer.0.SelfAttention.k.weight True\ndecoder.block.3.layer.0.SelfAttention.v.weight False\ndecoder.block.3.layer.0.SelfAttention.v.weight True\ndecoder.block.3.layer.0.SelfAttention.o.weight False\ndecoder.block.3.layer.0.SelfAttention.o.weight True\ndecoder.block.3.layer.0.layer_norm.weight False\ndecoder.block.3.layer.0.layer_norm.weight True\ndecoder.block.3.layer.1.EncDecAttention.q.weight False\ndecoder.block.3.layer.1.EncDecAttention.q.weight True\ndecoder.block.3.layer.1.EncDecAttention.k.weight False\ndecoder.block.3.layer.1.EncDecAttention.k.weight True\ndecoder.block.3.layer.1.EncDecAttention.v.weight False\ndecoder.block.3.layer.1.EncDecAttention.v.weight True\ndecoder.block.3.layer.1.EncDecAttention.o.weight False\ndecoder.block.3.layer.1.EncDecAttention.o.weight True\ndecoder.block.3.layer.1.layer_norm.weight False\ndecoder.block.3.layer.1.layer_norm.weight True\ndecoder.block.3.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.3.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.3.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.3.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.3.layer.2.DenseReluDense.wo.weight False\ndecoder.block.3.layer.2.DenseReluDense.wo.weight True\ndecoder.block.3.layer.2.layer_norm.weight False\ndecoder.block.3.layer.2.layer_norm.weight True\ndecoder.block.4.layer.0.SelfAttention.q.weight False\ndecoder.block.4.layer.0.SelfAttention.q.weight True\ndecoder.block.4.layer.0.SelfAttention.k.weight False\ndecoder.block.4.layer.0.SelfAttention.k.weight True\ndecoder.block.4.layer.0.SelfAttention.v.weight False\ndecoder.block.4.layer.0.SelfAttention.v.weight True\ndecoder.block.4.layer.0.SelfAttention.o.weight False\ndecoder.block.4.layer.0.SelfAttention.o.weight True\ndecoder.block.4.layer.0.layer_norm.weight False\ndecoder.block.4.layer.0.layer_norm.weight True\ndecoder.block.4.layer.1.EncDecAttention.q.weight False\ndecoder.block.4.layer.1.EncDecAttention.q.weight True\ndecoder.block.4.layer.1.EncDecAttention.k.weight False\ndecoder.block.4.layer.1.EncDecAttention.k.weight True\ndecoder.block.4.layer.1.EncDecAttention.v.weight False\ndecoder.block.4.layer.1.EncDecAttention.v.weight True\ndecoder.block.4.layer.1.EncDecAttention.o.weight False\ndecoder.block.4.layer.1.EncDecAttention.o.weight True\ndecoder.block.4.layer.1.layer_norm.weight False\ndecoder.block.4.layer.1.layer_norm.weight True\ndecoder.block.4.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.4.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.4.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.4.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.4.layer.2.DenseReluDense.wo.weight False\ndecoder.block.4.layer.2.DenseReluDense.wo.weight True\ndecoder.block.4.layer.2.layer_norm.weight False\ndecoder.block.4.layer.2.layer_norm.weight True\ndecoder.block.5.layer.0.SelfAttention.q.weight False\ndecoder.block.5.layer.0.SelfAttention.q.weight True\ndecoder.block.5.layer.0.SelfAttention.k.weight False\ndecoder.block.5.layer.0.SelfAttention.k.weight True\ndecoder.block.5.layer.0.SelfAttention.v.weight False\ndecoder.block.5.layer.0.SelfAttention.v.weight True\ndecoder.block.5.layer.0.SelfAttention.o.weight False\ndecoder.block.5.layer.0.SelfAttention.o.weight True\ndecoder.block.5.layer.0.layer_norm.weight False\ndecoder.block.5.layer.0.layer_norm.weight True\ndecoder.block.5.layer.1.EncDecAttention.q.weight False\ndecoder.block.5.layer.1.EncDecAttention.q.weight True\ndecoder.block.5.layer.1.EncDecAttention.k.weight False\ndecoder.block.5.layer.1.EncDecAttention.k.weight True\ndecoder.block.5.layer.1.EncDecAttention.v.weight False\ndecoder.block.5.layer.1.EncDecAttention.v.weight True\ndecoder.block.5.layer.1.EncDecAttention.o.weight False\ndecoder.block.5.layer.1.EncDecAttention.o.weight True\ndecoder.block.5.layer.1.layer_norm.weight False\ndecoder.block.5.layer.1.layer_norm.weight True\ndecoder.block.5.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.5.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.5.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.5.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.5.layer.2.DenseReluDense.wo.weight False\ndecoder.block.5.layer.2.DenseReluDense.wo.weight True\ndecoder.block.5.layer.2.layer_norm.weight False\ndecoder.block.5.layer.2.layer_norm.weight True\ndecoder.block.6.layer.0.SelfAttention.q.weight False\ndecoder.block.6.layer.0.SelfAttention.q.weight True\ndecoder.block.6.layer.0.SelfAttention.k.weight False\ndecoder.block.6.layer.0.SelfAttention.k.weight True\ndecoder.block.6.layer.0.SelfAttention.v.weight False\ndecoder.block.6.layer.0.SelfAttention.v.weight True\ndecoder.block.6.layer.0.SelfAttention.o.weight False\ndecoder.block.6.layer.0.SelfAttention.o.weight True\ndecoder.block.6.layer.0.layer_norm.weight False\ndecoder.block.6.layer.0.layer_norm.weight True\ndecoder.block.6.layer.1.EncDecAttention.q.weight False\ndecoder.block.6.layer.1.EncDecAttention.q.weight True\ndecoder.block.6.layer.1.EncDecAttention.k.weight False\ndecoder.block.6.layer.1.EncDecAttention.k.weight True\ndecoder.block.6.layer.1.EncDecAttention.v.weight False\ndecoder.block.6.layer.1.EncDecAttention.v.weight True\ndecoder.block.6.layer.1.EncDecAttention.o.weight False\ndecoder.block.6.layer.1.EncDecAttention.o.weight True\ndecoder.block.6.layer.1.layer_norm.weight False\ndecoder.block.6.layer.1.layer_norm.weight True\ndecoder.block.6.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.6.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.6.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.6.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.6.layer.2.DenseReluDense.wo.weight False\ndecoder.block.6.layer.2.DenseReluDense.wo.weight True\ndecoder.block.6.layer.2.layer_norm.weight False\ndecoder.block.6.layer.2.layer_norm.weight True\ndecoder.block.7.layer.0.SelfAttention.q.weight True\ndecoder.block.7.layer.0.SelfAttention.k.weight True\ndecoder.block.7.layer.0.SelfAttention.v.weight True\ndecoder.block.7.layer.0.SelfAttention.o.weight True\ndecoder.block.7.layer.0.layer_norm.weight True\ndecoder.block.7.layer.1.EncDecAttention.q.weight True\ndecoder.block.7.layer.1.EncDecAttention.k.weight True\ndecoder.block.7.layer.1.EncDecAttention.v.weight True\ndecoder.block.7.layer.1.EncDecAttention.o.weight True\ndecoder.block.7.layer.1.layer_norm.weight True\ndecoder.block.7.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.7.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.7.layer.2.DenseReluDense.wo.weight True\ndecoder.block.7.layer.2.layer_norm.weight True\ndecoder.final_layer_norm.weight True\nlm_head.weight True\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfor name, param in model.named_parameters():\n     print(name,param.requires_grad)\n     if name.startswith(\"encoder.block.0.layer\"): # choose whatever you like here\n        param.requires_grad = False\n        print(name,param.requires_grad)\n     for i in range (0,7):\n         if name.startswith(f\"decoder.block.{i}.layer\"): # choose whatever you like here\n            param.requires_grad = False\n            print(name,param.requires_grad)    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T09:44:37.102752Z","iopub.execute_input":"2024-03-20T09:44:37.103142Z","iopub.status.idle":"2024-03-20T09:44:37.185232Z","shell.execute_reply.started":"2024-03-20T09:44:37.103111Z","shell.execute_reply":"2024-03-20T09:44:37.184279Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"shared.weight False\nencoder.block.0.layer.0.SelfAttention.q.weight False\nencoder.block.0.layer.0.SelfAttention.q.weight False\nencoder.block.0.layer.0.SelfAttention.k.weight False\nencoder.block.0.layer.0.SelfAttention.k.weight False\nencoder.block.0.layer.0.SelfAttention.v.weight False\nencoder.block.0.layer.0.SelfAttention.v.weight False\nencoder.block.0.layer.0.SelfAttention.o.weight False\nencoder.block.0.layer.0.SelfAttention.o.weight False\nencoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\nencoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\nencoder.block.0.layer.0.layer_norm.weight False\nencoder.block.0.layer.0.layer_norm.weight False\nencoder.block.0.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.0.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.0.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.0.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.0.layer.1.DenseReluDense.wo.weight False\nencoder.block.0.layer.1.DenseReluDense.wo.weight False\nencoder.block.0.layer.1.layer_norm.weight False\nencoder.block.0.layer.1.layer_norm.weight False\nencoder.block.1.layer.0.SelfAttention.q.weight False\nencoder.block.1.layer.0.SelfAttention.k.weight False\nencoder.block.1.layer.0.SelfAttention.v.weight False\nencoder.block.1.layer.0.SelfAttention.o.weight False\nencoder.block.1.layer.0.layer_norm.weight False\nencoder.block.1.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.1.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.1.layer.1.DenseReluDense.wo.weight False\nencoder.block.1.layer.1.layer_norm.weight False\nencoder.block.2.layer.0.SelfAttention.q.weight False\nencoder.block.2.layer.0.SelfAttention.k.weight False\nencoder.block.2.layer.0.SelfAttention.v.weight False\nencoder.block.2.layer.0.SelfAttention.o.weight False\nencoder.block.2.layer.0.layer_norm.weight False\nencoder.block.2.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.2.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.2.layer.1.DenseReluDense.wo.weight False\nencoder.block.2.layer.1.layer_norm.weight False\nencoder.block.3.layer.0.SelfAttention.q.weight False\nencoder.block.3.layer.0.SelfAttention.k.weight False\nencoder.block.3.layer.0.SelfAttention.v.weight False\nencoder.block.3.layer.0.SelfAttention.o.weight False\nencoder.block.3.layer.0.layer_norm.weight False\nencoder.block.3.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.3.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.3.layer.1.DenseReluDense.wo.weight False\nencoder.block.3.layer.1.layer_norm.weight False\nencoder.block.4.layer.0.SelfAttention.q.weight False\nencoder.block.4.layer.0.SelfAttention.k.weight False\nencoder.block.4.layer.0.SelfAttention.v.weight False\nencoder.block.4.layer.0.SelfAttention.o.weight False\nencoder.block.4.layer.0.layer_norm.weight False\nencoder.block.4.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.4.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.4.layer.1.DenseReluDense.wo.weight False\nencoder.block.4.layer.1.layer_norm.weight False\nencoder.block.5.layer.0.SelfAttention.q.weight False\nencoder.block.5.layer.0.SelfAttention.k.weight False\nencoder.block.5.layer.0.SelfAttention.v.weight False\nencoder.block.5.layer.0.SelfAttention.o.weight False\nencoder.block.5.layer.0.layer_norm.weight False\nencoder.block.5.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.5.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.5.layer.1.DenseReluDense.wo.weight False\nencoder.block.5.layer.1.layer_norm.weight False\nencoder.block.6.layer.0.SelfAttention.q.weight False\nencoder.block.6.layer.0.SelfAttention.k.weight False\nencoder.block.6.layer.0.SelfAttention.v.weight False\nencoder.block.6.layer.0.SelfAttention.o.weight False\nencoder.block.6.layer.0.layer_norm.weight False\nencoder.block.6.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.6.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.6.layer.1.DenseReluDense.wo.weight False\nencoder.block.6.layer.1.layer_norm.weight False\nencoder.block.7.layer.0.SelfAttention.q.weight False\nencoder.block.7.layer.0.SelfAttention.k.weight False\nencoder.block.7.layer.0.SelfAttention.v.weight False\nencoder.block.7.layer.0.SelfAttention.o.weight False\nencoder.block.7.layer.0.layer_norm.weight False\nencoder.block.7.layer.1.DenseReluDense.wi_0.weight False\nencoder.block.7.layer.1.DenseReluDense.wi_1.weight False\nencoder.block.7.layer.1.DenseReluDense.wo.weight False\nencoder.block.7.layer.1.layer_norm.weight False\nencoder.final_layer_norm.weight False\ndecoder.block.0.layer.0.SelfAttention.q.weight False\ndecoder.block.0.layer.0.SelfAttention.q.weight False\ndecoder.block.0.layer.0.SelfAttention.k.weight False\ndecoder.block.0.layer.0.SelfAttention.k.weight False\ndecoder.block.0.layer.0.SelfAttention.v.weight False\ndecoder.block.0.layer.0.SelfAttention.v.weight False\ndecoder.block.0.layer.0.SelfAttention.o.weight False\ndecoder.block.0.layer.0.SelfAttention.o.weight False\ndecoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\ndecoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\ndecoder.block.0.layer.0.layer_norm.weight False\ndecoder.block.0.layer.0.layer_norm.weight False\ndecoder.block.0.layer.1.EncDecAttention.q.weight False\ndecoder.block.0.layer.1.EncDecAttention.q.weight False\ndecoder.block.0.layer.1.EncDecAttention.k.weight False\ndecoder.block.0.layer.1.EncDecAttention.k.weight False\ndecoder.block.0.layer.1.EncDecAttention.v.weight False\ndecoder.block.0.layer.1.EncDecAttention.v.weight False\ndecoder.block.0.layer.1.EncDecAttention.o.weight False\ndecoder.block.0.layer.1.EncDecAttention.o.weight False\ndecoder.block.0.layer.1.layer_norm.weight False\ndecoder.block.0.layer.1.layer_norm.weight False\ndecoder.block.0.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.0.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.0.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.0.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.0.layer.2.DenseReluDense.wo.weight False\ndecoder.block.0.layer.2.DenseReluDense.wo.weight False\ndecoder.block.0.layer.2.layer_norm.weight False\ndecoder.block.0.layer.2.layer_norm.weight False\ndecoder.block.1.layer.0.SelfAttention.q.weight True\ndecoder.block.1.layer.0.SelfAttention.q.weight False\ndecoder.block.1.layer.0.SelfAttention.k.weight True\ndecoder.block.1.layer.0.SelfAttention.k.weight False\ndecoder.block.1.layer.0.SelfAttention.v.weight True\ndecoder.block.1.layer.0.SelfAttention.v.weight False\ndecoder.block.1.layer.0.SelfAttention.o.weight True\ndecoder.block.1.layer.0.SelfAttention.o.weight False\ndecoder.block.1.layer.0.layer_norm.weight True\ndecoder.block.1.layer.0.layer_norm.weight False\ndecoder.block.1.layer.1.EncDecAttention.q.weight True\ndecoder.block.1.layer.1.EncDecAttention.q.weight False\ndecoder.block.1.layer.1.EncDecAttention.k.weight True\ndecoder.block.1.layer.1.EncDecAttention.k.weight False\ndecoder.block.1.layer.1.EncDecAttention.v.weight True\ndecoder.block.1.layer.1.EncDecAttention.v.weight False\ndecoder.block.1.layer.1.EncDecAttention.o.weight True\ndecoder.block.1.layer.1.EncDecAttention.o.weight False\ndecoder.block.1.layer.1.layer_norm.weight True\ndecoder.block.1.layer.1.layer_norm.weight False\ndecoder.block.1.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.1.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.1.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.1.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.1.layer.2.DenseReluDense.wo.weight True\ndecoder.block.1.layer.2.DenseReluDense.wo.weight False\ndecoder.block.1.layer.2.layer_norm.weight True\ndecoder.block.1.layer.2.layer_norm.weight False\ndecoder.block.2.layer.0.SelfAttention.q.weight True\ndecoder.block.2.layer.0.SelfAttention.q.weight False\ndecoder.block.2.layer.0.SelfAttention.k.weight True\ndecoder.block.2.layer.0.SelfAttention.k.weight False\ndecoder.block.2.layer.0.SelfAttention.v.weight True\ndecoder.block.2.layer.0.SelfAttention.v.weight False\ndecoder.block.2.layer.0.SelfAttention.o.weight True\ndecoder.block.2.layer.0.SelfAttention.o.weight False\ndecoder.block.2.layer.0.layer_norm.weight True\ndecoder.block.2.layer.0.layer_norm.weight False\ndecoder.block.2.layer.1.EncDecAttention.q.weight True\ndecoder.block.2.layer.1.EncDecAttention.q.weight False\ndecoder.block.2.layer.1.EncDecAttention.k.weight True\ndecoder.block.2.layer.1.EncDecAttention.k.weight False\ndecoder.block.2.layer.1.EncDecAttention.v.weight True\ndecoder.block.2.layer.1.EncDecAttention.v.weight False\ndecoder.block.2.layer.1.EncDecAttention.o.weight True\ndecoder.block.2.layer.1.EncDecAttention.o.weight False\ndecoder.block.2.layer.1.layer_norm.weight True\ndecoder.block.2.layer.1.layer_norm.weight False\ndecoder.block.2.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.2.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.2.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.2.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.2.layer.2.DenseReluDense.wo.weight True\ndecoder.block.2.layer.2.DenseReluDense.wo.weight False\ndecoder.block.2.layer.2.layer_norm.weight True\ndecoder.block.2.layer.2.layer_norm.weight False\ndecoder.block.3.layer.0.SelfAttention.q.weight True\ndecoder.block.3.layer.0.SelfAttention.q.weight False\ndecoder.block.3.layer.0.SelfAttention.k.weight True\ndecoder.block.3.layer.0.SelfAttention.k.weight False\ndecoder.block.3.layer.0.SelfAttention.v.weight True\ndecoder.block.3.layer.0.SelfAttention.v.weight False\ndecoder.block.3.layer.0.SelfAttention.o.weight True\ndecoder.block.3.layer.0.SelfAttention.o.weight False\ndecoder.block.3.layer.0.layer_norm.weight True\ndecoder.block.3.layer.0.layer_norm.weight False\ndecoder.block.3.layer.1.EncDecAttention.q.weight True\ndecoder.block.3.layer.1.EncDecAttention.q.weight False\ndecoder.block.3.layer.1.EncDecAttention.k.weight True\ndecoder.block.3.layer.1.EncDecAttention.k.weight False\ndecoder.block.3.layer.1.EncDecAttention.v.weight True\ndecoder.block.3.layer.1.EncDecAttention.v.weight False\ndecoder.block.3.layer.1.EncDecAttention.o.weight True\ndecoder.block.3.layer.1.EncDecAttention.o.weight False\ndecoder.block.3.layer.1.layer_norm.weight True\ndecoder.block.3.layer.1.layer_norm.weight False\ndecoder.block.3.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.3.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.3.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.3.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.3.layer.2.DenseReluDense.wo.weight True\ndecoder.block.3.layer.2.DenseReluDense.wo.weight False\ndecoder.block.3.layer.2.layer_norm.weight True\ndecoder.block.3.layer.2.layer_norm.weight False\ndecoder.block.4.layer.0.SelfAttention.q.weight True\ndecoder.block.4.layer.0.SelfAttention.q.weight False\ndecoder.block.4.layer.0.SelfAttention.k.weight True\ndecoder.block.4.layer.0.SelfAttention.k.weight False\ndecoder.block.4.layer.0.SelfAttention.v.weight True\ndecoder.block.4.layer.0.SelfAttention.v.weight False\ndecoder.block.4.layer.0.SelfAttention.o.weight True\ndecoder.block.4.layer.0.SelfAttention.o.weight False\ndecoder.block.4.layer.0.layer_norm.weight True\ndecoder.block.4.layer.0.layer_norm.weight False\ndecoder.block.4.layer.1.EncDecAttention.q.weight True\ndecoder.block.4.layer.1.EncDecAttention.q.weight False\ndecoder.block.4.layer.1.EncDecAttention.k.weight True\ndecoder.block.4.layer.1.EncDecAttention.k.weight False\ndecoder.block.4.layer.1.EncDecAttention.v.weight True\ndecoder.block.4.layer.1.EncDecAttention.v.weight False\ndecoder.block.4.layer.1.EncDecAttention.o.weight True\ndecoder.block.4.layer.1.EncDecAttention.o.weight False\ndecoder.block.4.layer.1.layer_norm.weight True\ndecoder.block.4.layer.1.layer_norm.weight False\ndecoder.block.4.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.4.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.4.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.4.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.4.layer.2.DenseReluDense.wo.weight True\ndecoder.block.4.layer.2.DenseReluDense.wo.weight False\ndecoder.block.4.layer.2.layer_norm.weight True\ndecoder.block.4.layer.2.layer_norm.weight False\ndecoder.block.5.layer.0.SelfAttention.q.weight True\ndecoder.block.5.layer.0.SelfAttention.q.weight False\ndecoder.block.5.layer.0.SelfAttention.k.weight True\ndecoder.block.5.layer.0.SelfAttention.k.weight False\ndecoder.block.5.layer.0.SelfAttention.v.weight True\ndecoder.block.5.layer.0.SelfAttention.v.weight False\ndecoder.block.5.layer.0.SelfAttention.o.weight True\ndecoder.block.5.layer.0.SelfAttention.o.weight False\ndecoder.block.5.layer.0.layer_norm.weight True\ndecoder.block.5.layer.0.layer_norm.weight False\ndecoder.block.5.layer.1.EncDecAttention.q.weight True\ndecoder.block.5.layer.1.EncDecAttention.q.weight False\ndecoder.block.5.layer.1.EncDecAttention.k.weight True\ndecoder.block.5.layer.1.EncDecAttention.k.weight False\ndecoder.block.5.layer.1.EncDecAttention.v.weight True\ndecoder.block.5.layer.1.EncDecAttention.v.weight False\ndecoder.block.5.layer.1.EncDecAttention.o.weight True\ndecoder.block.5.layer.1.EncDecAttention.o.weight False\ndecoder.block.5.layer.1.layer_norm.weight True\ndecoder.block.5.layer.1.layer_norm.weight False\ndecoder.block.5.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.5.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.5.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.5.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.5.layer.2.DenseReluDense.wo.weight True\ndecoder.block.5.layer.2.DenseReluDense.wo.weight False\ndecoder.block.5.layer.2.layer_norm.weight True\ndecoder.block.5.layer.2.layer_norm.weight False\ndecoder.block.6.layer.0.SelfAttention.q.weight True\ndecoder.block.6.layer.0.SelfAttention.q.weight False\ndecoder.block.6.layer.0.SelfAttention.k.weight True\ndecoder.block.6.layer.0.SelfAttention.k.weight False\ndecoder.block.6.layer.0.SelfAttention.v.weight True\ndecoder.block.6.layer.0.SelfAttention.v.weight False\ndecoder.block.6.layer.0.SelfAttention.o.weight True\ndecoder.block.6.layer.0.SelfAttention.o.weight False\ndecoder.block.6.layer.0.layer_norm.weight True\ndecoder.block.6.layer.0.layer_norm.weight False\ndecoder.block.6.layer.1.EncDecAttention.q.weight True\ndecoder.block.6.layer.1.EncDecAttention.q.weight False\ndecoder.block.6.layer.1.EncDecAttention.k.weight True\ndecoder.block.6.layer.1.EncDecAttention.k.weight False\ndecoder.block.6.layer.1.EncDecAttention.v.weight True\ndecoder.block.6.layer.1.EncDecAttention.v.weight False\ndecoder.block.6.layer.1.EncDecAttention.o.weight True\ndecoder.block.6.layer.1.EncDecAttention.o.weight False\ndecoder.block.6.layer.1.layer_norm.weight True\ndecoder.block.6.layer.1.layer_norm.weight False\ndecoder.block.6.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.6.layer.2.DenseReluDense.wi_0.weight False\ndecoder.block.6.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.6.layer.2.DenseReluDense.wi_1.weight False\ndecoder.block.6.layer.2.DenseReluDense.wo.weight True\ndecoder.block.6.layer.2.DenseReluDense.wo.weight False\ndecoder.block.6.layer.2.layer_norm.weight True\ndecoder.block.6.layer.2.layer_norm.weight False\ndecoder.block.7.layer.0.SelfAttention.q.weight True\ndecoder.block.7.layer.0.SelfAttention.k.weight True\ndecoder.block.7.layer.0.SelfAttention.v.weight True\ndecoder.block.7.layer.0.SelfAttention.o.weight True\ndecoder.block.7.layer.0.layer_norm.weight True\ndecoder.block.7.layer.1.EncDecAttention.q.weight True\ndecoder.block.7.layer.1.EncDecAttention.k.weight True\ndecoder.block.7.layer.1.EncDecAttention.v.weight True\ndecoder.block.7.layer.1.EncDecAttention.o.weight True\ndecoder.block.7.layer.1.layer_norm.weight True\ndecoder.block.7.layer.2.DenseReluDense.wi_0.weight True\ndecoder.block.7.layer.2.DenseReluDense.wi_1.weight True\ndecoder.block.7.layer.2.DenseReluDense.wo.weight True\ndecoder.block.7.layer.2.layer_norm.weight True\ndecoder.final_layer_norm.weight True\nlm_head.weight True\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}