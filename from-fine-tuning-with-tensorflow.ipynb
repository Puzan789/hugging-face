{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-02-07T11:55:06.470478Z","iopub.execute_input":"2024-02-07T11:55:06.471000Z","iopub.status.idle":"2024-02-07T11:55:07.400200Z","shell.execute_reply.started":"2024-02-07T11:55:06.470972Z","shell.execute_reply":"2024-02-07T11:55:07.399268Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFAutoModelForSequenceClassification\ncheckpoint=\"bert-base-cased\"\nmodel=TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)# positive and negative classs num_labels=2\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)# softmax launa bhanda aagadi logits bata  calculate garxa\nmodel.compile(optimizer=\"adam\",loss=loss)\nmodel.fit(\n    tf_train_dataset,\n    validation_data=tf_validation_dataset,\n    epochs=3\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T12:02:01.378971Z","iopub.execute_input":"2024-02-07T12:02:01.379357Z","iopub.status.idle":"2024-02-07T12:02:03.145679Z","shell.execute_reply.started":"2024-02-07T12:02:01.379326Z","shell.execute_reply":"2024-02-07T12:02:03.144953Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Learning Rate Scheduling With tensorflow","metadata":{}},{"cell_type":"markdown","source":"#### Certainly! Let's break it down in simpler terms:\n\n**Learning Rate:**\n- Imagine you're trying to find the lowest point in a hilly terrain, and you can take steps to move downhill. The learning rate is like the size of the steps you take.\n- A high learning rate means you take big steps. You might reach the bottom quickly, but you risk overshooting and missing the exact lowest point.\n- A low learning rate means you take small steps. You'll be more precise, but it might take a long time to reach the lowest point.\n\n**Decay:**\n- Now, let's say you start with big steps (high learning rate) but want to gradually reduce the step size as you get closer to the lowest point to ensure precision.\n- Learning rate decay is like saying, \"Okay, I'm going to decrease the size of my steps over time as I get nearer to the optimal solution.\"\n- It's a way to balance between quickly getting close to the solution and carefully fine-tuning when you're in the vicinity.\n\nIn short, learning rate is about how big your optimization steps are, and decay is about gradually making those steps smaller as you make progress. Together, they help your model efficiently find the best solution. ","metadata":{}},{"cell_type":"markdown","source":"The default learning rate for the Adam optimizer in Keras is typically set to 0.001.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers.schedules import PolynomialDecay\nnum_epochs=3\nnum_train_steps=len(tf_train_dataset)*num_epochs\nlr_scheduler=polynomialDecay(\n\n    initial_learning_rate=5e-5,end_learning_rate=0.0,decay_steps=num_train_steps\n)\n\nfrom tf.keras.optimizers import Adam\nopt=Adam(learning_rate=lr_scheduler)\nmodel.compile(loss=loss,optimizer=opt)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T12:15:04.034990Z","iopub.execute_input":"2024-02-07T12:15:04.035374Z","iopub.status.idle":"2024-02-07T12:15:04.080702Z","shell.execute_reply.started":"2024-02-07T12:15:04.035342Z","shell.execute_reply":"2024-02-07T12:15:04.079238Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolynomialDecay\n\u001b[1;32m      2\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 3\u001b[0m num_train_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mtf_train_dataset\u001b[49m)\u001b[38;5;241m*\u001b[39mnum_epochs\n\u001b[1;32m      4\u001b[0m lr_scheduler\u001b[38;5;241m=\u001b[39mpolynomialDecay(\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     initial_learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,end_learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,decay_steps\u001b[38;5;241m=\u001b[39mnum_train_steps\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n","\u001b[0;31mNameError\u001b[0m: name 'tf_train_dataset' is not defined"],"ename":"NameError","evalue":"name 'tf_train_dataset' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Tensorflow Predictions And Metrics","metadata":{}},{"cell_type":"code","source":"preds=model.predict(tokenized_datasets['validation'])['logits']\nprobabilities=tf.nn.softmax(preds)\nclass_preds=np.argmax(probabilities,axis=1)\nmodel.compile(loss=loss,opt=opt,metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}