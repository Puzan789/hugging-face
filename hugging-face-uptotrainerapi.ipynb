{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"509777da-092d-488d-9952-7f4cd339c7e5","_cell_guid":"8bb0b9d0-446f-48f4-be8d-5819a4e18375","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T07:03:24.030085Z","iopub.execute_input":"2024-02-06T07:03:24.030474Z","iopub.status.idle":"2024-02-06T07:03:41.206135Z","shell.execute_reply.started":"2024-02-06T07:03:24.030444Z","shell.execute_reply":"2024-02-06T07:03:41.204770Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import pipeline\nimport tensorflow as tf\nimport math","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:08:19.284848Z","iopub.execute_input":"2024-02-06T09:08:19.285211Z","iopub.status.idle":"2024-02-06T09:08:19.289872Z","shell.execute_reply.started":"2024-02-06T09:08:19.285181Z","shell.execute_reply":"2024-02-06T09:08:19.288914Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline Function","metadata":{}},{"cell_type":"code","source":"classifier=pipeline(\"sentiment-analysis\")\nclassifier([\"i have been working here more the time without paid\",\n          \"i hate everything and love everything\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:05:18.540381Z","iopub.execute_input":"2024-02-06T07:05:18.540793Z","iopub.status.idle":"2024-02-06T07:05:18.897502Z","shell.execute_reply.started":"2024-02-06T07:05:18.540764Z","shell.execute_reply":"2024-02-06T07:05:18.896330Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[{'label': 'NEGATIVE', 'score': 0.9956315755844116},\n {'label': 'POSITIVE', 'score': 0.999284565448761}]"},"metadata":{}}]},{"cell_type":"code","source":"classifier=pipeline(\"zero-shot-classification\")\nclassifier(\"the education system is bad in the  world due to politics\",\n          candidate_labels=['education','politics','environments'])","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:07:38.331712Z","iopub.execute_input":"2024-02-06T07:07:38.332110Z","iopub.status.idle":"2024-02-06T07:07:41.811486Z","shell.execute_reply.started":"2024-02-06T07:07:38.332083Z","shell.execute_reply":"2024-02-06T07:07:41.810210Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'sequence': 'the education system is bad in the  world due to politics',\n 'labels': ['education', 'politics', 'environments'],\n 'scores': [0.5001472234725952, 0.4958876967430115, 0.0039651351980865]}"},"metadata":{}}]},{"cell_type":"code","source":"generator=pipeline(\"text-generation\")\ngenerator(\"Nepal is a country having \")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:08:54.245924Z","iopub.execute_input":"2024-02-06T07:08:54.246365Z","iopub.status.idle":"2024-02-06T07:08:57.263092Z","shell.execute_reply.started":"2024-02-06T07:08:54.246290Z","shell.execute_reply":"2024-02-06T07:08:57.261838Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': 'Nepal is a country having \\xa0a strong ethnic and religious community. But is Tibet an independent country?\\xa0 Is Nepal one of the most beautiful, or is India its favourite country?\\xa0 Has India made peace with Tibet? There is a'}]"},"metadata":{}}]},{"cell_type":"code","source":"generator=pipeline(\"text-generation\",model=\"gpt2\")\ngenerator(\"Nepal is a country having \",max_length=30,num_return_sequences=3)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:11:08.569425Z","iopub.execute_input":"2024-02-06T07:11:08.569905Z","iopub.status.idle":"2024-02-06T07:11:11.179595Z","shell.execute_reply.started":"2024-02-06T07:11:08.569869Z","shell.execute_reply":"2024-02-06T07:11:11.178331Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': 'Nepal is a country having \\xa0its own identity crisis, and that crisis has been exacerbated by a rising tide of ethnic tensions. The '},\n {'generated_text': \"Nepal is a country having vernacular words and words of English used by its people in a way that the West doesn't understand. For\"},\n {'generated_text': 'Nepal is a country having _____ for all intents and purposes,\" says Vintner. And here is where your mileage crosses the line'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fill-mask for missing words\n","metadata":{}},{"cell_type":"code","source":"unmasker=pipeline(\"fill-mask\")\nunmasker(\"this world is best for <mask> and is not best for <mask>\",top_k=3)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:14:01.999723Z","iopub.execute_input":"2024-02-06T07:14:02.000749Z","iopub.status.idle":"2024-02-06T07:14:02.848662Z","shell.execute_reply.started":"2024-02-06T07:14:02.000708Z","shell.execute_reply":"2024-02-06T07:14:02.847704Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSome weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[[{'score': 0.06011602282524109,\n   'token': 961,\n   'token_str': ' everyone',\n   'sequence': '<s>this world is best for everyone and is not best for<mask></s>'},\n  {'score': 0.04559367895126343,\n   'token': 30515,\n   'token_str': ' mankind',\n   'sequence': '<s>this world is best for mankind and is not best for<mask></s>'},\n  {'score': 0.04126153886318207,\n   'token': 9187,\n   'token_str': ' humanity',\n   'sequence': '<s>this world is best for humanity and is not best for<mask></s>'}],\n [{'score': 0.05913162976503372,\n   'token': 961,\n   'token_str': ' everyone',\n   'sequence': '<s>this world is best for<mask> and is not best for everyone</s>'},\n  {'score': 0.05457741394639015,\n   'token': 9187,\n   'token_str': ' humanity',\n   'sequence': '<s>this world is best for<mask> and is not best for humanity</s>'},\n  {'score': 0.05069468170404434,\n   'token': 30515,\n   'token_str': ' mankind',\n   'sequence': '<s>this world is best for<mask> and is not best for mankind</s>'}]]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Named Entity recognition","metadata":{}},{"cell_type":"code","source":"ner=pipeline(\"ner\",grouped_entities=True)\nner(\"My name is Hero and i am from bangladesh i am an student of arts. I work at a huggingFace.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:19:16.041315Z","iopub.execute_input":"2024-02-06T07:19:16.041769Z","iopub.status.idle":"2024-02-06T07:19:16.967252Z","shell.execute_reply.started":"2024-02-06T07:19:16.041738Z","shell.execute_reply":"2024-02-06T07:19:16.965928Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSome weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[{'entity_group': 'PER',\n  'score': 0.99065137,\n  'word': 'Hero',\n  'start': 11,\n  'end': 15},\n {'entity_group': 'LOC',\n  'score': 0.79891044,\n  'word': 'bangladesh',\n  'start': 30,\n  'end': 40},\n {'entity_group': 'ORG',\n  'score': 0.6582433,\n  'word': '##Face',\n  'start': 85,\n  'end': 89}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Question-answering","metadata":{}},{"cell_type":"code","source":"ana=pipeline(\"question-answering\")\nana(\nquestion=\"where does the peoples work\",\ncontext=\"peoples work at the far region\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:20:27.639935Z","iopub.execute_input":"2024-02-06T07:20:27.640457Z","iopub.status.idle":"2024-02-06T07:20:32.103681Z","shell.execute_reply.started":"2024-02-06T07:20:27.640420Z","shell.execute_reply":"2024-02-06T07:20:32.102208Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c475fd5869dc4e288321fac0c9c96f60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae3d2b661d1432c8ead57df06228287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49fe4fe5935d447da3d7ca89bc73034f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc843ed91714f5a8178bc7c141ee46c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"862a8c03f6f249419837f787d966f10f"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'score': 0.44286495447158813,\n 'start': 16,\n 'end': 30,\n 'answer': 'the far region'}"},"metadata":{}}]},{"cell_type":"code","source":"summary=pipeline(\"summarization\")\nsummary(\"\"\" \nIn the enchanting realm of artificial intelligence, where algorithms dance with data, innovation unfolds at a breathtaking pace. From the realms of machine learning to natural language processing, the landscape of AI continually evolves, weaving intricate patterns of intellect. As silicon minds tirelessly crunch numbers, they give rise to technologies that reshape our world.\n\nAt the core of this digital odyssey lies the marvel of deep learning, an intricate web of interconnected neurons emulating the human brain. Neural networks, inspired by their biological counterparts, unravel complex problems with an unprecedented finesse. They dissect images, translate languages, and even compose symphonies, all driven by the relentless pursuit of efficiency.\n\nIn the ever-expanding universe of AI applications, natural language processing emerges as a luminary. Machines decipher the nuances of human communication, conversing fluently and comprehending context. Chatbots engage in dialogues, virtual assistants anticipate needs, and language models like GPT-3 unravel the beauty of language in profound ways. Yet, ethical considerations and responsible AI stewardship accompany these advancements, urging a delicate balance between progress and prudence.\n\nAs algorithms traverse the vast expanse of uncharted territories, the transformative power of AI extends beyond computation. Autonomous vehicles navigate bustling streets, healthcare welcomes the precision of predictive analytics, and industries embrace automation to enhance productivity. The tapestry of possibilities woven by artificial intelligence becomes a testament to human ingenuity, propelling society into an era where the boundaries of innovation are seemingly boundless.\n\nIn this tapestry, challenges intertwine with triumphs, as the quest for ethical AI, unbiased algorithms, and equitable access unfolds. The future of artificial intelligence is an unwritten symphony, where each line of code adds a note to the melody of progress. As the journey continues, humanity stands at the crossroads of discovery, holding the keys to unlock the vast potential embedded in the digital age.\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:21:59.755192Z","iopub.execute_input":"2024-02-06T07:21:59.755797Z","iopub.status.idle":"2024-02-06T07:22:18.934693Z","shell.execute_reply.started":"2024-02-06T07:21:59.755763Z","shell.execute_reply":"2024-02-06T07:22:18.933394Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34592790fd1b49d7a7271cd7431d38fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f507498aced4e10858fa52957021c14"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4cb9d2cc5245a2aaf6f85ea2a857dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbe2b356268477fba7d5889d58565f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42535cff11347859c6a89f5519d3aa4"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': ' The landscape of AI continually evolves, weaving intricate patterns of intellect . As silicon minds crunch numbers, they give rise to technologies that reshape our world . The future of artificial intelligence is an unwritten symphony, where each line of code adds a note to the melody of progress .'}]"},"metadata":{}}]},{"cell_type":"code","source":"translator=pipeline(\"translation\",model=\"google-t5/t5-large\")\ntranslator(\"the world is beautiful\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T07:25:22.696779Z","iopub.execute_input":"2024-02-06T07:25:22.698118Z","iopub.status.idle":"2024-02-06T07:25:46.305366Z","shell.execute_reply.started":"2024-02-06T07:25:22.698078Z","shell.execute_reply":"2024-02-06T07:25:46.304084Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5501ae97fb14fd79984a3f728b5924d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edec939a07634245abadd4842b872f74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e73e11a16e41ce97849893bd49372a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9420192bcf43969fd4851cbf5ff1a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e80b308e12fd43e59e8e6443dc1dedad"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1049: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n  warnings.warn(\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"[{'translation_text': 'die Welt ist schön'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# What happens inside a Pipeline Function?\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ncheckpoint=\"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer=AutoTokenizer.from_pretrained(checkpoint)\nraw_inputs=[\"i have been drinking from last 2 days\",'shut the fuck up']\ninputs=tokenizer(raw_inputs,padding=True,truncation=True,return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T08:57:23.796199Z","iopub.execute_input":"2024-02-06T08:57:23.796864Z","iopub.status.idle":"2024-02-06T08:57:23.965814Z","shell.execute_reply.started":"2024-02-06T08:57:23.796821Z","shell.execute_reply":"2024-02-06T08:57:23.964867Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"inputs['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2024-02-06T08:58:08.941375Z","iopub.execute_input":"2024-02-06T08:58:08.941757Z","iopub.status.idle":"2024-02-06T08:58:08.948227Z","shell.execute_reply.started":"2024-02-06T08:58:08.941727Z","shell.execute_reply":"2024-02-06T08:58:08.947368Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 10), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], dtype=int32)>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\ncheckpoint=\"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel=TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\noutput=model(**inputs)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:09:17.552039Z","iopub.execute_input":"2024-02-06T09:09:17.552393Z","iopub.status.idle":"2024-02-06T09:09:18.795436Z","shell.execute_reply.started":"2024-02-06T09:09:17.552363Z","shell.execute_reply":"2024-02-06T09:09:18.794441Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n\nAll the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"output.logits","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:10:27.579909Z","iopub.execute_input":"2024-02-06T09:10:27.580859Z","iopub.status.idle":"2024-02-06T09:10:27.588673Z","shell.execute_reply.started":"2024-02-06T09:10:27.580812Z","shell.execute_reply":"2024-02-06T09:10:27.587633Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[ 1.7742105, -1.5290266],\n       [ 3.1991885, -2.5411508]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"predictions=tf.math.softmax(output.logits,axis=-1)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:09:24.926794Z","iopub.execute_input":"2024-02-06T09:09:24.927616Z","iopub.status.idle":"2024-02-06T09:09:24.935179Z","shell.execute_reply.started":"2024-02-06T09:09:24.927583Z","shell.execute_reply":"2024-02-06T09:09:24.934222Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[[0.96453965 0.03546031]\n [0.99679667 0.00320338]], shape=(2, 2), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.id2label","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:16:51.335392Z","iopub.execute_input":"2024-02-06T09:16:51.335912Z","iopub.status.idle":"2024-02-06T09:16:51.343539Z","shell.execute_reply.started":"2024-02-06T09:16:51.335878Z","shell.execute_reply":"2024-02-06T09:16:51.342614Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{0: 'NEGATIVE', 1: 'POSITIVE'}"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming model is an instance of DistilBertConfig\nprint(model.config.__dict__)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:13:01.198932Z","iopub.execute_input":"2024-02-06T09:13:01.199806Z","iopub.status.idle":"2024-02-06T09:13:01.204534Z","shell.execute_reply.started":"2024-02-06T09:13:01.199768Z","shell.execute_reply":"2024-02-06T09:13:01.203557Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"{'vocab_size': 30522, 'max_position_embeddings': 512, 'sinusoidal_pos_embds': False, 'n_layers': 6, 'n_heads': 12, 'dim': 768, 'hidden_dim': 3072, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation': 'gelu', 'initializer_range': 0.02, 'qa_dropout': 0.1, 'seq_classif_dropout': 0.2, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['DistilBertForSequenceClassification'], 'finetuning_task': 'sst-2', 'id2label': {0: 'NEGATIVE', 1: 'POSITIVE'}, 'label2id': {'NEGATIVE': 0, 'POSITIVE': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'distilbert-base-uncased-finetuned-sst-2-english', '_commit_hash': '714eb0fa89d2f80546fda750413ed43d93601a13', '_attn_implementation_internal': None, 'transformers_version': None, 'model_type': 'distilbert', 'output_past': True, 'tie_weights_': True}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Instantiate a transformers Model","metadata":{}},{"cell_type":"code","source":"from transformers import TFAutoModel\nbert_model=TFAutoModel.from_pretrained(\"bert-base-cased\")\nprint(type(bert_model))\ngpt_model=TFAutoModel.from_pretrained(\"gpt2\")\nprint(type(gpt_model))\nbart_model=TFAutoModel.from_pretrained(\"facebook/bart-base\")\nprint(type(bart_model))","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:27:22.521452Z","iopub.execute_input":"2024-02-06T09:27:22.521838Z","iopub.status.idle":"2024-02-06T09:27:27.720752Z","shell.execute_reply.started":"2024-02-06T09:27:22.521807Z","shell.execute_reply":"2024-02-06T09:27:27.719925Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n","output_type":"stream"},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFGPT2Model.\n\nAll the weights of TFGPT2Model were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"<class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model'>\n","output_type":"stream"},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBartModel.\n\nAll the weights of TFBartModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"<class 'transformers.models.bart.modeling_tf_bart.TFBartModel'>\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoConfig\n\nbert_config=AutoConfig.from_pretrained(\"bert-base-cased\")\nprint(type(bert_config))\n\ngpt_config=AutoConfig.from_pretrained(\"gpt2\")\nprint(type(gpt_config))\nbart_config=AutoConfig.from_pretrained(\"facebook/bart-base\")\nprint(type(bart_config))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:35:06.801406Z","iopub.execute_input":"2024-02-06T09:35:06.802226Z","iopub.status.idle":"2024-02-06T09:35:07.173022Z","shell.execute_reply.started":"2024-02-06T09:35:06.802188Z","shell.execute_reply":"2024-02-06T09:35:07.172116Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"<class 'transformers.models.bert.configuration_bert.BertConfig'>\n<class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>\n<class 'transformers.models.bart.configuration_bart.BartConfig'>\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_config","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:35:10.323241Z","iopub.execute_input":"2024-02-06T09:35:10.323650Z","iopub.status.idle":"2024-02-06T09:35:10.331797Z","shell.execute_reply.started":"2024-02-06T09:35:10.323617Z","shell.execute_reply":"2024-02-06T09:35:10.330794Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"_name_or_path\": \"bert-base-cased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.37.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"# You can instantiate a given model with random weights from this config","metadata":{}},{"cell_type":"code","source":"from transformers import BertConfig,TFBertModel","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:49:29.271960Z","iopub.execute_input":"2024-02-06T09:49:29.272696Z","iopub.status.idle":"2024-02-06T09:49:29.277007Z","shell.execute_reply.started":"2024-02-06T09:49:29.272662Z","shell.execute_reply":"2024-02-06T09:49:29.276134Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"bert_config=BertConfig.from_pretrained('bert-base-cased',num_hidden_layers=10)\nbert_model=TFBertModel(bert_config)\nbert_model.save_pretrained(\"my_bert_model\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:52:00.304992Z","iopub.execute_input":"2024-02-06T09:52:00.305602Z","iopub.status.idle":"2024-02-06T09:52:00.617039Z","shell.execute_reply.started":"2024-02-06T09:52:00.305556Z","shell.execute_reply":"2024-02-06T09:52:00.615894Z"},"trusted":true},"execution_count":55,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m bert_config\u001b[38;5;241m=\u001b[39mBertConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m,num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m bert_model\u001b[38;5;241m=\u001b[39mTFBertModel(bert_config)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_bert_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:2442\u001b[0m, in \u001b[0;36mTFPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, saved_model, version, push_to_hub, signatures, max_shard_size, create_pr, safe_serialization, token, **kwargs)\u001b[0m\n\u001b[1;32m   2439\u001b[0m weights_name \u001b[38;5;241m=\u001b[39m SAFE_WEIGHTS_NAME \u001b[38;5;28;01mif\u001b[39;00m safe_serialization \u001b[38;5;28;01melse\u001b[39;00m TF2_WEIGHTS_NAME\n\u001b[1;32m   2440\u001b[0m output_model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, weights_name)\n\u001b[0;32m-> 2442\u001b[0m shards, index \u001b[38;5;241m=\u001b[39m tf_shard_checkpoint(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m, max_shard_size)\n\u001b[1;32m   2444\u001b[0m \u001b[38;5;66;03m# Clean the folder from a previous save\u001b[39;00m\n\u001b[1;32m   2445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(save_directory):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3454\u001b[0m, in \u001b[0;36mModel.weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   3447\u001b[0m \n\u001b[1;32m   3448\u001b[0m \u001b[38;5;124;03m    Note: This will not track the weights of nested `tf.Modules` that are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[38;5;124;03m      A list of variables.\u001b[39;00m\n\u001b[1;32m   3453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dedup_weights(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_undeduplicated_weights\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3459\u001b[0m, in \u001b[0;36mModel._undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_undeduplicated_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3458\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3459\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_weights_created\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3460\u001b[0m     weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_tracked_trackables:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3838\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   3829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3830\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m   3831\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m!=\u001b[39m Model\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3836\u001b[0m     \u001b[38;5;66;03m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[1;32m   3837\u001b[0m     \u001b[38;5;66;03m# defined.\u001b[39;00m\n\u001b[0;32m-> 3838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3839\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights for model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m have not yet been \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are created when the model is first called on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3843\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Weights for model 'tf_bert_model_5' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`."],"ename":"ValueError","evalue":"Weights for model 'tf_bert_model_5' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:51:53.514609Z","iopub.execute_input":"2024-02-06T09:51:53.515444Z","iopub.status.idle":"2024-02-06T09:51:53.616756Z","shell.execute_reply.started":"2024-02-06T09:51:53.515408Z","shell.execute_reply":"2024-02-06T09:51:53.615616Z"},"trusted":true},"execution_count":54,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_bert_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:2442\u001b[0m, in \u001b[0;36mTFPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, saved_model, version, push_to_hub, signatures, max_shard_size, create_pr, safe_serialization, token, **kwargs)\u001b[0m\n\u001b[1;32m   2439\u001b[0m weights_name \u001b[38;5;241m=\u001b[39m SAFE_WEIGHTS_NAME \u001b[38;5;28;01mif\u001b[39;00m safe_serialization \u001b[38;5;28;01melse\u001b[39;00m TF2_WEIGHTS_NAME\n\u001b[1;32m   2440\u001b[0m output_model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, weights_name)\n\u001b[0;32m-> 2442\u001b[0m shards, index \u001b[38;5;241m=\u001b[39m tf_shard_checkpoint(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m, max_shard_size)\n\u001b[1;32m   2444\u001b[0m \u001b[38;5;66;03m# Clean the folder from a previous save\u001b[39;00m\n\u001b[1;32m   2445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(save_directory):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3454\u001b[0m, in \u001b[0;36mModel.weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   3447\u001b[0m \n\u001b[1;32m   3448\u001b[0m \u001b[38;5;124;03m    Note: This will not track the weights of nested `tf.Modules` that are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[38;5;124;03m      A list of variables.\u001b[39;00m\n\u001b[1;32m   3453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dedup_weights(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_undeduplicated_weights\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3459\u001b[0m, in \u001b[0;36mModel._undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_undeduplicated_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3458\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3459\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_weights_created\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3460\u001b[0m     weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_tracked_trackables:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3838\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   3829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3830\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m   3831\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m!=\u001b[39m Model\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3836\u001b[0m     \u001b[38;5;66;03m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[1;32m   3837\u001b[0m     \u001b[38;5;66;03m# defined.\u001b[39;00m\n\u001b[0;32m-> 3838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3839\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights for model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m have not yet been \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are created when the model is first called on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3843\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Weights for model 'tf_bert_model_4' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`."],"ename":"ValueError","evalue":"Weights for model 'tf_bert_model_4' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.","output_type":"error"}]},{"cell_type":"code","source":"import json\n\n# Specify the path to the JSON file\nfile_path = '/kaggle/working/my_bert_model/config.json'\n\n# Read the contents of the file\nwith open(file_path, 'r') as file:\n    json_content = file.read()\n\n# Parse the JSON content\nro = json.loads(json_content)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:54:01.942187Z","iopub.execute_input":"2024-02-06T09:54:01.942816Z","iopub.status.idle":"2024-02-06T09:54:01.948138Z","shell.execute_reply.started":"2024-02-06T09:54:01.942779Z","shell.execute_reply":"2024-02-06T09:54:01.947219Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"ro","metadata":{"execution":{"iopub.status.busy":"2024-02-06T09:54:06.151504Z","iopub.execute_input":"2024-02-06T09:54:06.151902Z","iopub.status.idle":"2024-02-06T09:54:06.158504Z","shell.execute_reply.started":"2024-02-06T09:54:06.151872Z","shell.execute_reply":"2024-02-06T09:54:06.157613Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"{'architectures': ['BertModel'],\n 'attention_probs_dropout_prob': 0.1,\n 'classifier_dropout': None,\n 'gradient_checkpointing': False,\n 'hidden_act': 'gelu',\n 'hidden_dropout_prob': 0.1,\n 'hidden_size': 768,\n 'initializer_range': 0.02,\n 'intermediate_size': 3072,\n 'layer_norm_eps': 1e-12,\n 'max_position_embeddings': 512,\n 'model_type': 'bert',\n 'num_attention_heads': 12,\n 'num_hidden_layers': 10,\n 'pad_token_id': 0,\n 'position_embedding_type': 'absolute',\n 'transformers_version': '4.37.0',\n 'type_vocab_size': 2,\n 'use_cache': True,\n 'vocab_size': 28996}"},"metadata":{}}]},{"cell_type":"markdown","source":"# wordbased character-based,subword_based","metadata":{}},{"cell_type":"markdown","source":"# The Tokenization Pipeline","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:04:04.919289Z","iopub.execute_input":"2024-02-06T13:04:04.920223Z","iopub.status.idle":"2024-02-06T13:04:04.924554Z","shell.execute_reply.started":"2024-02-06T13:04:04.920183Z","shell.execute_reply":"2024-02-06T13:04:04.923574Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\ntokens=tokenizer.tokenize(\"let's try by tokenizing\")\n\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:46:35.359948Z","iopub.execute_input":"2024-02-06T12:46:35.360617Z","iopub.status.idle":"2024-02-06T12:46:41.741248Z","shell.execute_reply.started":"2024-02-06T12:46:35.360578Z","shell.execute_reply":"2024-02-06T12:46:41.740265Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"803aaa863e5f443295a523bd6006c68d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97403dec273e46d5b5876fba4cc74681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65325dd8b101489ba8a954dfea98071b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"183e39569d934a489f22205084c560ec"}},"metadata":{}},{"name":"stdout","text":"['let', \"'\", 's', 'try', 'by', 'token', '##izing']\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\ntokens=tokenizer.tokenize(\"let's try by tokenizing\")\ninput_ids=tokenizer.convert_tokens_to_ids(tokens)\nprint(input_ids)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:49:21.928816Z","iopub.execute_input":"2024-02-06T12:49:21.929508Z","iopub.status.idle":"2024-02-06T12:49:22.063001Z","shell.execute_reply.started":"2024-02-06T12:49:21.929477Z","shell.execute_reply":"2024-02-06T12:49:22.062072Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[2292, 1005, 1055, 3046, 2011, 19204, 6026]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\ntokens=tokenizer.tokenize(\"let's try by tokenizing\")\ninput_ids=tokenizer.convert_tokens_to_ids(tokens)\nfinal_output=tokenizer.prepare_for_model(input_ids)\nprint(final_output)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:56:34.195515Z","iopub.execute_input":"2024-02-06T12:56:34.196240Z","iopub.status.idle":"2024-02-06T12:56:34.345287Z","shell.execute_reply.started":"2024-02-06T12:56:34.196207Z","shell.execute_reply":"2024-02-06T12:56:34.344224Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"{'input_ids': [101, 2292, 1005, 1055, 3046, 2011, 19204, 6026, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### use .decode to decode the","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ninputs=tokenizer(\"let's try to tokenize and decoding the exa larger words by tokenization\")\nprint(tokenizer.decode(inputs[\"input_ids\"]))","metadata":{"execution":{"iopub.status.busy":"2024-02-06T12:55:27.141875Z","iopub.execute_input":"2024-02-06T12:55:27.142782Z","iopub.status.idle":"2024-02-06T12:55:27.277088Z","shell.execute_reply.started":"2024-02-06T12:55:27.142749Z","shell.execute_reply":"2024-02-06T12:55:27.276200Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[CLS] let's try to tokenize and decoding the exa larger words by tokenization [SEP]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Batching input Together","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ncheckpoint=\"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer=AutoTokenizer.from_pretrained(checkpoint)\nsentences=[\n    \"I've been holidayy in a intense holiday for a couple of months\",\n    \"I hate the holiday\"\n]\ntokens=[tokenizer.tokenize(sent) for sent in sentences]\nids=[tokenizer.convert_tokens_to_ids(tok) for tok in tokens]\nids","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:03:29.333731Z","iopub.execute_input":"2024-02-06T13:03:29.334102Z","iopub.status.idle":"2024-02-06T13:03:29.500523Z","shell.execute_reply.started":"2024-02-06T13:03:29.334073Z","shell.execute_reply":"2024-02-06T13:03:29.499618Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[[1045,\n  1005,\n  2310,\n  2042,\n  6209,\n  2100,\n  1999,\n  1037,\n  6387,\n  6209,\n  2005,\n  1037,\n  3232,\n  1997,\n  2706],\n [1045, 5223, 1996, 6209]]"},"metadata":{}}]},{"cell_type":"code","source":"ids0=tf.constant(ids[0])\nids1=tf.constant([1045, 5223, 1996, 6209,0,0,0,0,0,0,0,0,0,0,0])\nprint(ids1)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:21:24.024303Z","iopub.execute_input":"2024-02-06T13:21:24.024696Z","iopub.status.idle":"2024-02-06T13:21:24.030976Z","shell.execute_reply.started":"2024-02-06T13:21:24.024667Z","shell.execute_reply":"2024-02-06T13:21:24.030092Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[1045 5223 1996 6209    0    0    0    0    0    0    0    0    0    0\n    0], shape=(15,), dtype=int32)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(ids1))\nprint(len(ids0))","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:21:32.584096Z","iopub.execute_input":"2024-02-06T13:21:32.584573Z","iopub.status.idle":"2024-02-06T13:21:32.589567Z","shell.execute_reply.started":"2024-02-06T13:21:32.584541Z","shell.execute_reply":"2024-02-06T13:21:32.588641Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"15\n15\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TFAutoModelForSequenceClassification\nmodel=TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\noutput1=model(ids0)\noutput2=model(ids1)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:21:42.394866Z","iopub.execute_input":"2024-02-06T13:21:42.395220Z","iopub.status.idle":"2024-02-06T13:21:45.093059Z","shell.execute_reply.started":"2024-02-06T13:21:42.395191Z","shell.execute_reply":"2024-02-06T13:21:45.092250Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n\nAll the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(output1.logits)\nprint(output2.logits)\no","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:22:02.620782Z","iopub.execute_input":"2024-02-06T13:22:02.621505Z","iopub.status.idle":"2024-02-06T13:22:02.628035Z","shell.execute_reply.started":"2024-02-06T13:22:02.621475Z","shell.execute_reply":"2024-02-06T13:22:02.626499Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"tf.Tensor([[-1.4933891  1.5724165]], shape=(1, 2), dtype=float32)\ntf.Tensor([[ 1.7307717 -1.567487 ]], shape=(1, 2), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hugging Face Datasets overview","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nraw_datasets=load_dataset(\"glue\",'mrpc')\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:27:10.935782Z","iopub.execute_input":"2024-02-06T13:27:10.936912Z","iopub.status.idle":"2024-02-06T13:27:13.649699Z","shell.execute_reply.started":"2024-02-06T13:27:10.936879Z","shell.execute_reply":"2024-02-06T13:27:13.648823Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7bbc0987d24beea772d1d9902132ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8f34824e794cdf8e698efa3669f004"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fda0d5638cd41b1a63e561c5388d9b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c908b3d18c844f9c93f7fcd4a07d203a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f50698c44b04344b98e4d11049e1685"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d0217bc22fb4f08932d630cca13b285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6bb2f531a864efcba24f939a41fdca6"}},"metadata":{}},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets.keys()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:27:47.527235Z","iopub.execute_input":"2024-02-06T13:27:47.528111Z","iopub.status.idle":"2024-02-06T13:27:47.535390Z","shell.execute_reply.started":"2024-02-06T13:27:47.528076Z","shell.execute_reply":"2024-02-06T13:27:47.534330Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"dict_keys(['train', 'validation', 'test'])"},"metadata":{}}]},{"cell_type":"code","source":"df=raw_datasets['train']","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:29:14.330401Z","iopub.execute_input":"2024-02-06T13:29:14.330775Z","iopub.status.idle":"2024-02-06T13:29:14.335207Z","shell.execute_reply.started":"2024-02-06T13:29:14.330745Z","shell.execute_reply":"2024-02-06T13:29:14.334269Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.DataFrame(df)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:29:16.298966Z","iopub.execute_input":"2024-02-06T13:29:16.299331Z","iopub.status.idle":"2024-02-06T13:29:16.602818Z","shell.execute_reply.started":"2024-02-06T13:29:16.299294Z","shell.execute_reply":"2024-02-06T13:29:16.601963Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"                                              sentence1  \\\n0     Amrozi accused his brother , whom he called \" ...   \n1     Yucaipa owned Dominick 's before selling the c...   \n2     They had published an advertisement on the Int...   \n3     Around 0335 GMT , Tab shares were up 19 cents ...   \n4     The stock rose $ 2.11 , or about 11 percent , ...   \n...                                                 ...   \n3663  \" At this point , Mr. Brando announced : ' Som...   \n3664  Martin , 58 , will be freed today after servin...   \n3665  \" We have concluded that the outlook for price...   \n3666  The notification was first reported Friday by ...   \n3667  The 30-year bond US30YT = RR rose 22 / 32 for ...   \n\n                                              sentence2  label   idx  \n0     Referring to him as only \" the witness \" , Amr...      1     0  \n1     Yucaipa bought Dominick 's in 1995 for $ 693 m...      0     1  \n2     On June 10 , the ship 's owners had published ...      1     2  \n3     Tab shares jumped 20 cents , or 4.6 % , to set...      0     3  \n4     PG & E Corp. shares jumped $ 1.63 or 8 percent...      1     4  \n...                                                 ...    ...   ...  \n3663  Brando said that \" somebody ought to put a bul...      1  4071  \n3664  Martin served two thirds of a five-year senten...      0  4072  \n3665  In a statement , the ECB said the outlook for ...      1  4073  \n3666  MSNBC.com first reported the CIA request on Fr...      1  4074  \n3667  The 30-year bond US30YT = RR grew 1-3 / 32 for...      0  4075  \n\n[3668 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>label</th>\n      <th>idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Amrozi accused his brother , whom he called \" ...</td>\n      <td>Referring to him as only \" the witness \" , Amr...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yucaipa owned Dominick 's before selling the c...</td>\n      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>They had published an advertisement on the Int...</td>\n      <td>On June 10 , the ship 's owners had published ...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3663</th>\n      <td>\" At this point , Mr. Brando announced : ' Som...</td>\n      <td>Brando said that \" somebody ought to put a bul...</td>\n      <td>1</td>\n      <td>4071</td>\n    </tr>\n    <tr>\n      <th>3664</th>\n      <td>Martin , 58 , will be freed today after servin...</td>\n      <td>Martin served two thirds of a five-year senten...</td>\n      <td>0</td>\n      <td>4072</td>\n    </tr>\n    <tr>\n      <th>3665</th>\n      <td>\" We have concluded that the outlook for price...</td>\n      <td>In a statement , the ECB said the outlook for ...</td>\n      <td>1</td>\n      <td>4073</td>\n    </tr>\n    <tr>\n      <th>3666</th>\n      <td>The notification was first reported Friday by ...</td>\n      <td>MSNBC.com first reported the CIA request on Fr...</td>\n      <td>1</td>\n      <td>4074</td>\n    </tr>\n    <tr>\n      <th>3667</th>\n      <td>The 30-year bond US30YT = RR rose 22 / 32 for ...</td>\n      <td>The 30-year bond US30YT = RR grew 1-3 / 32 for...</td>\n      <td>0</td>\n      <td>4075</td>\n    </tr>\n  </tbody>\n</table>\n<p>3668 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:35:24.754662Z","iopub.execute_input":"2024-02-06T13:35:24.755560Z","iopub.status.idle":"2024-02-06T13:35:24.761129Z","shell.execute_reply.started":"2024-02-06T13:35:24.755528Z","shell.execute_reply":"2024-02-06T13:35:24.760174Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ncheckpoint=\"bert-base-cased\"\nmodel=AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_function(example):\n    return tokenizer(example['sentence1'],example['sentence2'],padding=\"max_length\",truncation=True,max_length=128)\ntokenized_datasets=raw_datasets.map(tokenize_function,batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:43:18.638621Z","iopub.execute_input":"2024-02-06T13:43:18.638982Z","iopub.status.idle":"2024-02-06T13:43:19.620137Z","shell.execute_reply.started":"2024-02-06T13:43:18.638952Z","shell.execute_reply":"2024-02-06T13:43:19.619417Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f4aaea7e6048ac95649ec77ff9bc27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f505a232d2744349c8b39168a6cd0c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"298d05c6893f41b09fb5e6dac5b517d1"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets.column_names","metadata":{"execution":{"iopub.status.busy":"2024-02-06T15:52:25.786939Z","iopub.execute_input":"2024-02-06T15:52:25.787616Z","iopub.status.idle":"2024-02-06T15:52:26.113223Z","shell.execute_reply.started":"2024-02-06T15:52:25.787582Z","shell.execute_reply":"2024-02-06T15:52:26.111975Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenized_datasets\u001b[49m\u001b[38;5;241m.\u001b[39mcolumn_names\n","\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"],"ename":"NameError","evalue":"name 'tokenized_datasets' is not defined","output_type":"error"}]},{"cell_type":"code","source":"tokenized_datasets=tokenized_datasets.remove_columns([\"idx\",\"sentence1\",\"sentence2\"])\ntokenized_datasets=tokenized_datasets.rename_column(\"label\",\"labels\")\ntokenize_datasets=tokenized_datasets.with_format(\"tensorflow\")\ntokenized_datasets[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:43:32.886442Z","iopub.execute_input":"2024-02-06T13:43:32.887041Z","iopub.status.idle":"2024-02-06T13:43:32.905756Z","shell.execute_reply.started":"2024-02-06T13:43:32.887008Z","shell.execute_reply":"2024-02-06T13:43:32.904959Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 3668\n})"},"metadata":{}}]},{"cell_type":"code","source":"op=pd.DataFrame(tokenized_datasets['train'])\nop","metadata":{"execution":{"iopub.status.busy":"2024-02-06T13:44:33.287006Z","iopub.execute_input":"2024-02-06T13:44:33.287607Z","iopub.status.idle":"2024-02-06T13:44:34.218332Z","shell.execute_reply.started":"2024-02-06T13:44:33.287575Z","shell.execute_reply":"2024-02-06T13:44:34.217395Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"      labels                                          input_ids  \\\n0          1  [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010...   \n1          0  [101, 9805, 3540, 11514, 2050, 3079, 11282, 22...   \n2          1  [101, 2027, 2018, 2405, 2019, 15147, 2006, 199...   \n3          0  [101, 2105, 6021, 19481, 13938, 2102, 1010, 21...   \n4          1  [101, 1996, 4518, 3123, 1002, 1016, 1012, 2340...   \n...      ...                                                ...   \n3663       1  [101, 1000, 2012, 2023, 2391, 1010, 2720, 1012...   \n3664       0  [101, 3235, 1010, 5388, 1010, 2097, 2022, 1065...   \n3665       1  [101, 1000, 2057, 2031, 5531, 2008, 1996, 1768...   \n3666       1  [101, 1996, 26828, 2001, 2034, 2988, 5958, 201...   \n3667       0  [101, 1996, 2382, 1011, 2095, 5416, 2149, 1414...   \n\n                                         attention_mask  \n0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n...                                                 ...  \n3663  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3664  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3665  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3666  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3667  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n\n[3668 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[101, 9805, 3540, 11514, 2050, 3079, 11282, 22...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>[101, 2027, 2018, 2405, 2019, 15147, 2006, 199...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>[101, 2105, 6021, 19481, 13938, 2102, 1010, 21...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[101, 1996, 4518, 3123, 1002, 1016, 1012, 2340...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3663</th>\n      <td>1</td>\n      <td>[101, 1000, 2012, 2023, 2391, 1010, 2720, 1012...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3664</th>\n      <td>0</td>\n      <td>[101, 3235, 1010, 5388, 1010, 2097, 2022, 1065...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3665</th>\n      <td>1</td>\n      <td>[101, 1000, 2057, 2031, 5531, 2008, 1996, 1768...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3666</th>\n      <td>1</td>\n      <td>[101, 1996, 26828, 2001, 2034, 2988, 5958, 201...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3667</th>\n      <td>0</td>\n      <td>[101, 1996, 2382, 1011, 2095, 5416, 2149, 1414...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3668 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import AutoTokenizer\ncheckpoint=\"bert-base-uncased\"\ntokenizer=AutoTokenizer.from_pretrained(checkpoint)\nsequence=[\n    \"Ive been working harder for more than fucking 2 years \",\n    \"Help the world to make a better world\"\n]\nbatch=tokenizer(sequence,padding=True,truncation=True,return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:22:49.673361Z","iopub.execute_input":"2024-02-06T16:22:49.674106Z","iopub.status.idle":"2024-02-06T16:22:49.827594Z","shell.execute_reply.started":"2024-02-06T16:22:49.674077Z","shell.execute_reply":"2024-02-06T16:22:49.826622Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"batch","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:14:27.268680Z","iopub.execute_input":"2024-02-06T16:14:27.269304Z","iopub.status.idle":"2024-02-06T16:14:27.275819Z","shell.execute_reply.started":"2024-02-06T16:14:27.269273Z","shell.execute_reply":"2024-02-06T16:14:27.274968Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'input_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\narray([[ 101, 4921, 2063, 2042, 2551, 6211, 2005, 2062, 2084, 8239, 1016,\n        2086,  102],\n       [ 101, 2393, 1996, 2088, 2000, 2191, 1037, 2488, 2088,  102,    0,\n           0,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], dtype=int32)>}"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer\nraw_datasets=load_dataset(\"glue\",\"mrpc\")\ncheckpoint=\"bert-base-cased\"\ntokenizer=AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['sentence1'],example['sentence2'],padding=\"max_length\",truncation=True,max_length=128\n    )\ntokenized_datasets=raw_datasets.remove_columns([\"idx\",\"sentence1\",\"sentence2\"])\ntokenized_datasets=tokenized_datasets.rename_column(\"label\",\"labels\")\ntokenize_datasets=tokenized_datasets.with_format(\"tensorflow\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:22:05.381656Z","iopub.execute_input":"2024-02-06T16:22:05.382549Z","iopub.status.idle":"2024-02-06T16:22:06.035301Z","shell.execute_reply.started":"2024-02-06T16:22:05.382515Z","shell.execute_reply":"2024-02-06T16:22:06.034409Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e799034a3d0845d0bd8028f941295464"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(3668, 1)"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets[\"train\"].shape","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:22:12.552239Z","iopub.execute_input":"2024-02-06T16:22:12.552607Z","iopub.status.idle":"2024-02-06T16:22:12.558683Z","shell.execute_reply.started":"2024-02-06T16:22:12.552579Z","shell.execute_reply":"2024-02-06T16:22:12.557768Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(3668, 1)"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.utils import DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:24:18.134749Z","iopub.execute_input":"2024-02-06T16:24:18.135142Z","iopub.status.idle":"2024-02-06T16:24:18.279110Z","shell.execute_reply.started":"2024-02-06T16:24:18.135114Z","shell.execute_reply":"2024-02-06T16:24:18.277872Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'DataLoader' from 'tensorflow.keras.utils' (/opt/conda/lib/python3.10/site-packages/keras/api/_v2/keras/utils/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'DataLoader' from 'tensorflow.keras.utils' (/opt/conda/lib/python3.10/site-packages/keras/api/_v2/keras/utils/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:26:05.895793Z","iopub.execute_input":"2024-02-06T16:26:05.896644Z","iopub.status.idle":"2024-02-06T16:26:05.900537Z","shell.execute_reply.started":"2024-02-06T16:26:05.896613Z","shell.execute_reply":"2024-02-06T16:26:05.899590Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## The Trainer API","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer ,DataCollatorWithPadding\nraw_datasets=load_dataset(\"glue\",\"mrpc\")\ncheckpoint=\"bert-base-cased\"\ntokenizer=AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence1'],examples['sentence2'],truncation=True)\n\ntokenized_datasets=raw_datasets.map(tokenize_function,batched=True)\ndatacollator=DataCollatorWithPadding(tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:51:37.267152Z","iopub.execute_input":"2024-02-06T16:51:37.267516Z","iopub.status.idle":"2024-02-06T16:51:38.764161Z","shell.execute_reply.started":"2024-02-06T16:51:37.267489Z","shell.execute_reply":"2024-02-06T16:51:38.763236Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1294ce0bbf45119c3e2b0d7e35d098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e7ad1ce6554d8cb0dc97b546a7efd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da8f27f48bf6480b88c322b8f3e2f086"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f076ed64bcb640a1b4798f9efad8b51a"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom datasets import load_metric\nmetric=load_metric(\"glue\",\"mrpc\")\ndef compute_metrices(eval_preds):\n    logits,labels=eval_preds \n    preds=np.argmax(logits,axis=-1)\n    return metric.compute(predictions=preds,reference=predictions.labels)\nmodel=AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2,max_length=64, gradient_checkpointing=True)\ntraining_args=TrainingArguments(\"test-trainer\",\n                               per_device_train_batch_size=4,\n                               per_device_eval_batch_size=4,\n                               num_train_epochs=2,\n                               learning_rate=2e-5,\n                               weight_decay=0.01,\n                                )\n\ntrainer=Trainer(\nmodel,\ntraining_args,\ntrain_dataset=tokenized_datasets['train'],\neval_dataset=tokenized_datasets['validation'],\ndata_collator=datacollator,\ntokenizer=tokenizer,\ncompute_metrics=compute_metrices)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:12:29.847693Z","iopub.execute_input":"2024-02-06T17:12:29.848601Z","iopub.status.idle":"2024-02-06T17:14:56.375486Z","shell.execute_reply.started":"2024-02-06T17:12:29.848559Z","shell.execute_reply":"2024-02-06T17:14:56.374585Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1834' max='1834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1834/1834 02:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.624100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.560400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.534100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory test-trainer/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCheckpoint destination directory test-trainer/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCheckpoint destination directory test-trainer/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 64}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1834, training_loss=0.5667965971136562, metrics={'train_runtime': 145.104, 'train_samples_per_second': 50.557, 'train_steps_per_second': 12.639, 'total_flos': 259374056151840.0, 'train_loss': 0.5667965971136562, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:00:38.705129Z","iopub.execute_input":"2024-02-06T17:00:38.706143Z","iopub.status.idle":"2024-02-06T17:00:39.205564Z","shell.execute_reply.started":"2024-02-06T17:00:38.706099Z","shell.execute_reply":"2024-02-06T17:00:39.204021Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming predictions is a dictionary with keys 'predictions' and 'label_ids'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# You may need to adapt this based on the actual structure of your predictions\u001b[39;00m\n\u001b[1;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43myour_predictions_array\u001b[49m,  \u001b[38;5;66;03m# Replace with your actual predictions array\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: your_label_ids_array,  \u001b[38;5;66;03m# Replace with your actual label_ids array\u001b[39;00m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Use np.argmax along the last axis\u001b[39;00m\n\u001b[1;32m     15\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'your_predictions_array' is not defined"],"ename":"NameError","evalue":"name 'your_predictions_array' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from datasets import load_metric\nmetric=load_metric(\"glue\",\"mrpc\")\npreds=np.argmax(predictions.predictions,axis=-1)\nmetric.compute(predictions=preds,reference=predictions.label_ids)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:59:05.973530Z","iopub.execute_input":"2024-02-06T16:59:05.973882Z","iopub.status.idle":"2024-02-06T16:59:06.678905Z","shell.execute_reply.started":"2024-02-06T16:59:05.973855Z","shell.execute_reply":"2024-02-06T16:59:06.677683Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m metric\u001b[38;5;241m=\u001b[39mload_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m preds\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mpredictions,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_ids\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/metric.py:419\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/metric.py:463\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m batch \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: predictions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m: references, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    462\u001b[0m batch \u001b[38;5;241m=\u001b[39m {intput_name: batch[intput_name] \u001b[38;5;28;01mfor\u001b[39;00m intput_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[0;32m--> 463\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1373\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1372\u001b[0m     column \u001b[38;5;241m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1373\u001b[0m     encoded_batch[key] \u001b[38;5;241m=\u001b[39m [encode_nested_example(\u001b[38;5;28mself\u001b[39m[key], obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"],"ename":"TypeError","evalue":"'NoneType' object is not iterable","output_type":"error"}]},{"cell_type":"code","source":"preds","metadata":{"execution":{"iopub.status.busy":"2024-02-06T16:58:30.287864Z","iopub.execute_input":"2024-02-06T16:58:30.288598Z","iopub.status.idle":"2024-02-06T16:58:30.298357Z","shell.execute_reply.started":"2024-02-06T16:58:30.288564Z","shell.execute_reply":"2024-02-06T16:58:30.297086Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}